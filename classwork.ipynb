{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seminar 6: Recurrent\tNeural\tNetworks and\tNatural\tLanguage\tProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Decoding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfolding a RNN in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Layers in Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing\tOur\tFirst\tRecurrent\tNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow\tsupports\tvarious\tvariants\tof\n",
    "RNNs\tthat\tcan\tbe\tfound\tin\tthe\t `tf.nn.rnn_cell` \tmodule.\tWith\tthe\t `tf.nn.dynamic_rnn()` \toperation,\n",
    "TensorFlow\talso\timplements\tthe\tRNN\tdynamics\tfor\tus.\n",
    "There\tis\talso\ta\tversion\tof\tthis\tfunction\tthat\tadds\tthe\tunfolded\toperations\tto\tthe\tgraph\n",
    "instead\tof\tusing\ta\tloop.\tHowever,\tthis\tconsumes\tconsiderably\tmore\tmemory\tand\thas\tno\n",
    "real\tbenefits.\tWe\ttherefore\tuse\tthe\tnewer\t `dynamic_rnn()` \toperation.\n",
    "As\tparameters,\t `dynamic_rnn()` \ttakes\ta\trecurrent\tnetwork\tdefinition\tand\tthe\tbatch\tof\tinput\n",
    "sequences.\tFor\tnow,\tthe\tsequences\tall\thave\tthe\tsame\tlength.\tThe\tfunction\tcreates\tthe\n",
    "required\tcomputations\tfor\tthe\tRNN\tto\tthe\tcompute\tgraph\tand\treturns\ttwo\ttensors\tholding\n",
    "the\toutputs\tand\thidden\tstates\tat\teach\ttime\tstep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The input data has dimensions batch_size * sequence_length * frame_size.\n",
    "# To not restrict ourselves to a fixed batch size, we use None as size of\n",
    "# the first dimension.\n",
    "sequence_length = 1440\n",
    "frame_size = 10\n",
    "data = tf.placeholder(tf.float32, [None, sequence_length, frame_size])\n",
    "\n",
    "num_neurons = 200\n",
    "network = tf.contrib.rnn.BasicRNNCell(num_neurons)\n",
    "# Define the operations to simulate the RNN for sequence_length steps.\n",
    "outputs, states = tf.nn.dynamic_rnn(network, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "\n",
    "class ImdbMovieReviews:\n",
    "    \"\"\"\n",
    "    The movie review dataset is offered by Stanford Universityâ€™s AI department:\n",
    "    http://ai.stanford.edu/~amaas/data/sentiment/. It comes as a compressed  tar  archive where\n",
    "    positive and negative reviews can be found as text files in two according folders. We apply\n",
    "    the same pre-processing to the text as in the last section: Extracting plain words using a\n",
    "    regular expression and converting to lower case.\n",
    "    \"\"\"\n",
    "    DEFAULT_URL = \\\n",
    "        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_dir = 'imdb'\n",
    "        self._url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "        \n",
    "        if not os.path.isfile(self._cache_dir):\n",
    "            urllib.request.urlretrieve(self._url, self._cache_dir)\n",
    "        self.filepath = self._cache_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self.filepath) as archive:\n",
    "            items = archive.getnames()\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN_REGEX.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\tcode\tshould\tbe\tstraight\tforward.\tWe\tjust\tuse\tthe\tvocabulary\tto\tdetermine\tthe\tindex\tof\n",
    "a\tword\tand\tuse\tthat\tindex\tto\tfind\tthe\tright\tembedding\tvector.\tThe\tfollowing\tclass\talso\n",
    "padds\tthe\tsequences\tto\tthe\tsame\tlength\tso\twe\tcan\teasily\tfit\tbatches\tof\tmultiple\treviews\n",
    "into\tyour\tnetwork\tlater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\n",
    "from spacy.en import English\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self, length):\n",
    "#          spaCy makes using word vectors very easy. \n",
    "#             The Lexeme , Token , Span  and Doc  classes all have a .vector property,\n",
    "#             which is a 1-dimensional numpy array of 32-bit floats:\n",
    "        self.parser = English()\n",
    "        self._length = length\n",
    "        self.dimensions = 300\n",
    "        \n",
    "    def __call__(self, sequence):\n",
    "        data = np.zeros((self._length, self.dimensions))\n",
    "        # you can access known words from the parser's vocabulary\n",
    "        embedded = [self.parser.vocab[w].vector for w in sequence]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence\tLabelling\tModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\twant\tto\tclassify\tthe\tsentiment\tof\ttext\tsequences.\tBecause\tthis\tis\ta\tsupervised\n",
    "setting,\twe\tpass\ttwo\tplaceholders\tto\tthe\tmodel:\tone\tfor\tthe\tinput\t data ,\tor\tthe\tsequence,\tand\n",
    "one\tfor\tthe\t target \tvalue,\tor\tthe\tsentiment.\tWe\talso\tpass\tin\tthe\t params \tobject\tthat\tcontains\n",
    "configuration\tparameters\tlike\tthe\tsize\tof\tthe\trecurrent\tlayer,\tits\tcell\tarchitecture\t(LSTM,\n",
    "GRU,\tetc),\tand\tthe\toptimizer\tto\tuse.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lazy import lazy\n",
    "\n",
    "class SequenceClassificationModel:\n",
    "    def __init__(self, data, params):\n",
    "        self.params = params\n",
    "        self._create_placeholders()\n",
    "        self.prediction\n",
    "        self.cost\n",
    "        self.error\n",
    "        self.optimize\n",
    "        self._create_summaries()\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.data = tf.placeholder(tf.float32, [None, self.params.seq_length, self.params.embed_length])\n",
    "            self.target = tf.placeholder(tf.float32, [None, 2])\n",
    "  \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar('loss', self.cost)\n",
    "            tf.summary.scalar('erroe', self.error)\n",
    "            self.summary = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "    @lazy\n",
    "    def length(self):\n",
    "    # First, we obtain the lengths of sequences in the current data batch. We need this since\n",
    "    # the data comes as a single tensor, padded with zero vectors to the longest review length.\n",
    "    # Instead of keeping track of the sequence lengths of every review, we just compute it\n",
    "    # dynamically in TensorFlow.\n",
    "    \n",
    "        with tf.name_scope(\"seq_length\"):\n",
    "            used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "            length = tf.reduce_sum(used, reduction_indices=1)\n",
    "            length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @lazy\n",
    "    def prediction(self):\n",
    "    # Note that the last relevant output activation of the RNN has a different index for each\n",
    "    # sequence in the training batch. This is because each review has a different length. We\n",
    "    # already know the length of each sequence.\n",
    "    # The problem is that we want to index in the dimension of time steps, which is\n",
    "    # the second dimension in the batch of shape  (sequences, time_steps, word_vectors) .\n",
    "    \n",
    "        with tf.name_scope(\"recurrent_layer\"):\n",
    "            output, _ = tf.nn.dynamic_rnn(\n",
    "                self.params.rnn_cell(self.params.rnn_hidden),\n",
    "                self.data,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=self.length\n",
    "            )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "\n",
    "        with tf.name_scope(\"softmax_layer\"):\n",
    "            num_classes = int(self.target.get_shape()[1])\n",
    "            weight = tf.Variable(tf.truncated_normal(\n",
    "                [self.params.rnn_hidden, num_classes], stddev=0.01))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "    \n",
    "    @lazy\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "    \n",
    "    @lazy\n",
    "    def error(self):\n",
    "        self.mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "    \n",
    "    @lazy\n",
    "    def optimize(self):\n",
    "    # RNNs are quite hard to train and weights tend to diverge if the hyper parameters do not\n",
    "    # play nicely together. The idea of gradient clipping is to restrict the the values of the\n",
    "    # gradient to a sensible range. This way, we can limit the maximum weight updates.\n",
    "\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            gradient = self.params.optimizer.compute_gradients(self.cost)\n",
    "            if self.params.gradient_clipping:\n",
    "                limit = self.params.gradient_clipping\n",
    "                gradient = [\n",
    "                    (tf.clip_by_value(g, -limit, limit), v)\n",
    "                    if g is not None else (None, v)\n",
    "                    for g, v in gradient]\n",
    "            optimize = self.params.optimizer.apply_gradients(gradient)\n",
    "        return optimize\n",
    "    \n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        with tf.name_scope(\"last_relevant\"):\n",
    "            # As of now, TensorFlow only supports indexing along the first dimension, using\n",
    "            # tf.gather() . We thus flatten the first two dimensions of the output activations from their\n",
    "            # shape of  sequences x time_steps x word_vectors  and construct an index into this resulting tensor.\n",
    "            batch_size = tf.shape(output)[0]\n",
    "            max_length = int(output.get_shape()[1])\n",
    "            output_size = int(output.get_shape()[2])\n",
    "\n",
    "            # The index takes into account the start indices for each sequence in the flat tensor and adds\n",
    "            # the sequence length to it. Actually, we only add  length - 1  so that we select the last valid\n",
    "            # time step.\n",
    "            index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "            flat = tf.reshape(output, [-1, output_size])\n",
    "            relevant = tf.gather(flat, index)\n",
    "        return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_batched(iterator, length, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        data = np.zeros((batch_size, length, embedding.dimensions))\n",
    "        target = np.zeros((batch_size, 2))\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            data[index] = embedding(text)\n",
    "            target[index] = [1, 0] if label else [0, 1]\n",
    "        yield data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = list(ImdbMovieReviews())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = max(len(x[0]) for x in reviews)\n",
    "embedding = Embedding(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from attrdict import AttrDict\n",
    "\n",
    "params = AttrDict(\n",
    "    rnn_cell=tf.contrib.rnn.GRUCell,\n",
    "    rnn_hidden=300,\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.002),\n",
    "    batch_size=20,\n",
    "    gradient_clipping=100,\n",
    "    seq_length=length,\n",
    "    embed_length=embedding.dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = preprocess_batched(reviews, length, embedding, params.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kurbanov/Soft/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = SequenceClassificationModel(data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 45.0%\n",
      "2: 50.0%\n",
      "3: 25.0%\n",
      "4: 70.0%\n",
      "5: 30.0%\n",
      "6: 40.0%\n",
      "7: 55.0%\n",
      "8: 50.0%\n",
      "9: 40.0%\n",
      "10: 60.0%\n",
      "11: 40.0%\n",
      "12: 40.0%\n",
      "13: 35.0%\n",
      "14: 65.0%\n",
      "15: 50.0%\n",
      "16: 55.0%\n",
      "17: 60.0%\n",
      "18: 45.0%\n",
      "19: 70.0%\n",
      "20: 55.0%\n",
      "21: 55.0%\n",
      "22: 45.0%\n",
      "23: 45.0%\n",
      "24: 60.0%\n",
      "25: 55.0%\n",
      "26: 55.0%\n",
      "27: 50.0%\n",
      "28: 35.0%\n",
      "29: 45.0%\n",
      "30: 35.0%\n",
      "31: 55.0%\n",
      "32: 60.0%\n",
      "33: 40.0%\n",
      "34: 65.0%\n",
      "35: 35.0%\n",
      "36: 45.0%\n",
      "37: 45.0%\n",
      "38: 50.0%\n",
      "39: 70.0%\n",
      "40: 65.0%\n",
      "41: 45.0%\n",
      "42: 60.0%\n",
      "43: 45.0%\n",
      "44: 50.0%\n",
      "45: 60.0%\n",
      "46: 60.0%\n",
      "47: 30.0%\n",
      "48: 70.0%\n",
      "49: 50.0%\n",
      "50: 40.0%\n",
      "51: 60.0%\n",
      "52: 35.0%\n",
      "53: 60.0%\n",
      "54: 55.0%\n",
      "55: 45.0%\n",
      "56: 35.0%\n",
      "57: 45.0%\n",
      "58: 50.0%\n",
      "59: 45.0%\n",
      "60: 55.0%\n",
      "61: 25.0%\n",
      "62: 45.0%\n",
      "63: 45.0%\n",
      "64: 50.0%\n",
      "65: 35.0%\n",
      "66: 55.0%\n",
      "67: 40.0%\n",
      "68: 45.0%\n",
      "69: 60.0%\n",
      "70: 35.0%\n",
      "71: 60.0%\n",
      "72: 45.0%\n",
      "73: 65.0%\n",
      "74: 55.0%\n",
      "75: 60.0%\n",
      "76: 55.0%\n",
      "77: 60.0%\n",
      "78: 25.0%\n",
      "79: 55.0%\n",
      "80: 40.0%\n",
      "81: 35.0%\n",
      "82: 30.0%\n",
      "83: 50.0%\n",
      "84: 40.0%\n",
      "85: 35.0%\n",
      "86: 65.0%\n",
      "87: 65.0%\n",
      "88: 50.0%\n",
      "89: 40.0%\n",
      "90: 35.0%\n",
      "91: 40.0%\n",
      "92: 40.0%\n",
      "93: 35.0%\n",
      "94: 40.0%\n",
      "95: 40.0%\n",
      "96: 45.0%\n",
      "97: 60.0%\n",
      "98: 60.0%\n",
      "99: 40.0%\n",
      "100: 20.0%\n",
      "101: 30.0%\n",
      "102: 15.0%\n",
      "103: 60.0%\n",
      "104: 65.0%\n",
      "105: 40.0%\n",
      "106: 30.0%\n",
      "107: 50.0%\n",
      "108: 50.0%\n",
      "109: 50.0%\n",
      "110: 50.0%\n",
      "111: 60.0%\n",
      "112: 50.0%\n",
      "113: 35.0%\n",
      "114: 50.0%\n",
      "115: 50.0%\n",
      "116: 35.0%\n",
      "117: 55.0%\n",
      "118: 35.0%\n",
      "119: 30.0%\n",
      "120: 40.0%\n",
      "121: 15.0%\n",
      "122: 35.0%\n",
      "123: 45.0%\n",
      "124: 50.0%\n",
      "125: 50.0%\n",
      "126: 65.0%\n",
      "127: 60.0%\n",
      "128: 45.0%\n",
      "129: 60.0%\n",
      "130: 45.0%\n",
      "131: 55.0%\n",
      "132: 35.0%\n",
      "133: 60.0%\n",
      "134: 40.0%\n",
      "135: 30.0%\n",
      "136: 45.0%\n",
      "137: 60.0%\n",
      "138: 40.0%\n",
      "139: 50.0%\n",
      "140: 30.0%\n",
      "141: 60.0%\n",
      "142: 50.0%\n",
      "143: 50.0%\n",
      "144: 55.0%\n",
      "145: 40.0%\n",
      "146: 50.0%\n",
      "147: 50.0%\n",
      "148: 35.0%\n",
      "149: 45.0%\n",
      "150: 30.0%\n",
      "151: 40.0%\n",
      "152: 45.0%\n",
      "153: 50.0%\n",
      "154: 25.0%\n",
      "155: 45.0%\n",
      "156: 40.0%\n",
      "157: 65.0%\n",
      "158: 40.0%\n",
      "159: 55.0%\n",
      "160: 40.0%\n",
      "161: 40.0%\n",
      "162: 45.0%\n",
      "163: 55.0%\n",
      "164: 45.0%\n",
      "165: 50.0%\n",
      "166: 35.0%\n",
      "167: 45.0%\n",
      "168: 30.0%\n",
      "169: 50.0%\n",
      "170: 45.0%\n",
      "171: 70.0%\n",
      "172: 50.0%\n",
      "173: 30.0%\n",
      "174: 50.0%\n",
      "175: 30.0%\n",
      "176: 45.0%\n",
      "177: 45.0%\n",
      "178: 45.0%\n",
      "179: 40.0%\n",
      "180: 50.0%\n",
      "181: 60.0%\n",
      "182: 25.0%\n",
      "183: 45.0%\n",
      "184: 65.0%\n",
      "185: 30.0%\n",
      "186: 65.0%\n",
      "187: 55.0%\n",
      "188: 50.0%\n",
      "189: 25.0%\n",
      "190: 45.0%\n",
      "191: 40.0%\n",
      "192: 50.0%\n",
      "193: 55.0%\n",
      "194: 40.0%\n",
      "195: 50.0%\n",
      "196: 45.0%\n",
      "197: 40.0%\n",
      "198: 25.0%\n",
      "199: 45.0%\n",
      "200: 45.0%\n",
      "201: 45.0%\n",
      "202: 50.0%\n",
      "203: 30.0%\n",
      "204: 25.0%\n",
      "205: 25.0%\n",
      "206: 55.0%\n",
      "207: 45.0%\n",
      "208: 60.0%\n",
      "209: 55.0%\n",
      "210: 25.0%\n",
      "211: 40.0%\n",
      "212: 35.0%\n",
      "213: 40.0%\n",
      "214: 40.0%\n",
      "215: 35.0%\n",
      "216: 40.0%\n",
      "217: 30.0%\n",
      "218: 50.0%\n",
      "219: 40.0%\n",
      "220: 25.0%\n",
      "221: 25.0%\n",
      "222: 30.0%\n",
      "223: 50.0%\n",
      "224: 5.0%\n",
      "225: 30.0%\n",
      "226: 30.0%\n",
      "227: 45.0%\n",
      "228: 55.0%\n",
      "229: 35.0%\n",
      "230: 30.0%\n",
      "231: 25.0%\n",
      "232: 20.0%\n",
      "233: 20.0%\n",
      "234: 40.0%\n",
      "235: 25.0%\n",
      "236: 40.0%\n",
      "237: 25.0%\n",
      "238: 30.0%\n",
      "239: 35.0%\n",
      "240: 30.0%\n",
      "241: 10.0%\n",
      "242: 25.0%\n",
      "243: 15.0%\n",
      "244: 35.0%\n",
      "245: 60.0%\n",
      "246: 35.0%\n",
      "247: 15.0%\n",
      "248: 15.0%\n",
      "249: 40.0%\n",
      "250: 15.0%\n",
      "251: 10.0%\n",
      "252: 10.0%\n",
      "253: 45.0%\n",
      "254: 15.0%\n",
      "255: 40.0%\n",
      "256: 20.0%\n",
      "257: 15.0%\n",
      "258: 20.0%\n",
      "259: 35.0%\n",
      "260: 35.0%\n",
      "261: 20.0%\n",
      "262: 15.0%\n",
      "263: 20.0%\n",
      "264: 10.0%\n",
      "265: 25.0%\n",
      "266: 30.0%\n",
      "267: 35.0%\n",
      "268: 30.0%\n",
      "269: 35.0%\n",
      "270: 25.0%\n",
      "271: 25.0%\n",
      "272: 30.0%\n",
      "273: 30.0%\n",
      "274: 30.0%\n",
      "275: 30.0%\n",
      "276: 15.0%\n",
      "277: 30.0%\n",
      "278: 10.0%\n",
      "279: 30.0%\n",
      "280: 20.0%\n",
      "281: 5.0%\n",
      "282: 10.0%\n",
      "283: 20.0%\n",
      "284: 10.0%\n",
      "285: 15.0%\n",
      "286: 25.0%\n",
      "287: 15.0%\n",
      "288: 30.0%\n",
      "289: 25.0%\n",
      "290: 35.0%\n",
      "291: 30.0%\n",
      "292: 15.0%\n",
      "293: 0.0%\n",
      "294: 20.0%\n",
      "295: 10.0%\n",
      "296: 25.0%\n",
      "297: 25.0%\n",
      "298: 5.0%\n",
      "299: 20.0%\n",
      "300: 30.0%\n",
      "301: 15.0%\n",
      "302: 20.0%\n",
      "303: 15.0%\n",
      "304: 15.0%\n",
      "305: 25.0%\n",
      "306: 30.0%\n",
      "307: 25.0%\n",
      "308: 15.0%\n",
      "309: 20.0%\n",
      "310: 25.0%\n",
      "311: 30.0%\n",
      "312: 25.0%\n",
      "313: 20.0%\n",
      "314: 20.0%\n",
      "315: 15.0%\n",
      "316: 20.0%\n",
      "317: 5.0%\n",
      "318: 35.0%\n",
      "319: 25.0%\n",
      "320: 15.0%\n",
      "321: 15.0%\n",
      "322: 30.0%\n",
      "323: 20.0%\n",
      "324: 25.0%\n",
      "325: 30.0%\n",
      "326: 10.0%\n",
      "327: 20.0%\n",
      "328: 20.0%\n",
      "329: 30.0%\n",
      "330: 10.0%\n",
      "331: 35.0%\n",
      "332: 15.0%\n",
      "333: 5.0%\n",
      "334: 0.0%\n",
      "335: 30.0%\n",
      "336: 25.0%\n",
      "337: 30.0%\n",
      "338: 15.0%\n",
      "339: 25.0%\n",
      "340: 20.0%\n",
      "341: 10.0%\n",
      "342: 30.0%\n",
      "343: 20.0%\n",
      "344: 15.0%\n",
      "345: 10.0%\n",
      "346: 30.0%\n",
      "347: 5.0%\n",
      "348: 25.0%\n",
      "349: 20.0%\n",
      "350: 20.0%\n",
      "351: 35.0%\n",
      "352: 20.0%\n",
      "353: 10.0%\n",
      "354: 15.0%\n",
      "355: 0.0%\n",
      "356: 5.0%\n",
      "357: 20.0%\n",
      "358: 20.0%\n",
      "359: 10.0%\n",
      "360: 5.0%\n",
      "361: 10.0%\n",
      "362: 20.0%\n",
      "363: 25.0%\n",
      "364: 15.0%\n",
      "365: 10.0%\n",
      "366: 15.0%\n",
      "367: 30.0%\n",
      "368: 35.0%\n",
      "369: 15.0%\n",
      "370: 20.0%\n",
      "371: 5.0%\n",
      "372: 10.0%\n",
      "373: 25.0%\n",
      "374: 10.0%\n",
      "375: 35.0%\n",
      "376: 25.0%\n",
      "377: 20.0%\n",
      "378: 25.0%\n",
      "379: 50.0%\n",
      "380: 30.0%\n",
      "381: 20.0%\n",
      "382: 10.0%\n",
      "383: 15.0%\n",
      "384: 15.0%\n",
      "385: 20.0%\n",
      "386: 20.0%\n",
      "387: 35.0%\n",
      "388: 20.0%\n",
      "389: 25.0%\n",
      "390: 20.0%\n",
      "391: 20.0%\n",
      "392: 30.0%\n",
      "393: 10.0%\n",
      "394: 35.0%\n",
      "395: 30.0%\n",
      "396: 20.0%\n",
      "397: 20.0%\n",
      "398: 10.0%\n",
      "399: 25.0%\n",
      "400: 20.0%\n",
      "401: 10.0%\n",
      "402: 30.0%\n",
      "403: 35.0%\n",
      "404: 40.0%\n",
      "405: 10.0%\n",
      "406: 30.0%\n",
      "407: 15.0%\n",
      "408: 10.0%\n",
      "409: 10.0%\n",
      "410: 5.0%\n",
      "411: 10.0%\n",
      "412: 5.0%\n",
      "413: 30.0%\n",
      "414: 35.0%\n",
      "415: 15.0%\n",
      "416: 25.0%\n",
      "417: 20.0%\n",
      "418: 20.0%\n",
      "419: 15.0%\n",
      "420: 15.0%\n",
      "421: 20.0%\n",
      "422: 20.0%\n",
      "423: 15.0%\n",
      "424: 20.0%\n",
      "425: 25.0%\n",
      "426: 10.0%\n",
      "427: 30.0%\n",
      "428: 25.0%\n",
      "429: 30.0%\n",
      "430: 15.0%\n",
      "431: 15.0%\n",
      "432: 10.0%\n",
      "433: 10.0%\n",
      "434: 20.0%\n",
      "435: 10.0%\n",
      "436: 15.0%\n",
      "437: 15.0%\n",
      "438: 15.0%\n",
      "439: 20.0%\n",
      "440: 10.0%\n",
      "441: 15.0%\n",
      "442: 25.0%\n",
      "443: 10.0%\n",
      "444: 35.0%\n",
      "445: 25.0%\n",
      "446: 30.0%\n",
      "447: 10.0%\n",
      "448: 15.0%\n",
      "449: 20.0%\n",
      "450: 10.0%\n",
      "451: 10.0%\n",
      "452: 20.0%\n",
      "453: 15.0%\n",
      "454: 10.0%\n",
      "455: 15.0%\n",
      "456: 15.0%\n",
      "457: 15.0%\n",
      "458: 15.0%\n",
      "459: 20.0%\n",
      "460: 5.0%\n",
      "461: 20.0%\n",
      "462: 15.0%\n",
      "463: 15.0%\n",
      "464: 15.0%\n",
      "465: 25.0%\n",
      "466: 15.0%\n",
      "467: 5.0%\n",
      "468: 10.0%\n",
      "469: 25.0%\n",
      "470: 30.0%\n",
      "471: 15.0%\n",
      "472: 30.0%\n",
      "473: 10.0%\n",
      "474: 15.0%\n",
      "475: 10.0%\n",
      "476: 10.0%\n",
      "477: 15.0%\n",
      "478: 10.0%\n",
      "479: 50.0%\n",
      "480: 10.0%\n",
      "481: 10.0%\n",
      "482: 15.0%\n",
      "483: 35.0%\n",
      "484: 25.0%\n",
      "485: 30.0%\n",
      "486: 20.0%\n",
      "487: 25.0%\n",
      "488: 0.0%\n",
      "489: 20.0%\n",
      "490: 25.0%\n",
      "491: 15.0%\n",
      "492: 25.0%\n",
      "493: 0.0%\n",
      "494: 20.0%\n",
      "495: 30.0%\n",
      "496: 25.0%\n",
      "497: 10.0%\n",
      "498: 25.0%\n",
      "499: 25.0%\n",
      "500: 25.0%\n",
      "501: 15.0%\n",
      "502: 35.0%\n",
      "503: 15.0%\n",
      "504: 20.0%\n",
      "505: 15.0%\n",
      "506: 5.0%\n",
      "507: 25.0%\n",
      "508: 15.0%\n",
      "509: 5.0%\n",
      "510: 5.0%\n",
      "511: 10.0%\n",
      "512: 20.0%\n",
      "513: 30.0%\n",
      "514: 20.0%\n",
      "515: 20.0%\n",
      "516: 35.0%\n",
      "517: 5.0%\n",
      "518: 15.0%\n",
      "519: 20.0%\n",
      "520: 20.0%\n",
      "521: 20.0%\n",
      "522: 15.0%\n",
      "523: 45.0%\n",
      "524: 20.0%\n",
      "525: 30.0%\n",
      "526: 20.0%\n",
      "527: 10.0%\n",
      "528: 20.0%\n",
      "529: 10.0%\n",
      "530: 15.0%\n",
      "531: 10.0%\n",
      "532: 20.0%\n",
      "533: 10.0%\n",
      "534: 15.0%\n",
      "535: 15.0%\n",
      "536: 10.0%\n",
      "537: 10.0%\n",
      "538: 10.0%\n",
      "539: 25.0%\n",
      "540: 30.0%\n",
      "541: 25.0%\n",
      "542: 25.0%\n",
      "543: 15.0%\n",
      "544: 20.0%\n",
      "545: 25.0%\n",
      "546: 20.0%\n",
      "547: 25.0%\n",
      "548: 15.0%\n",
      "549: 20.0%\n",
      "550: 20.0%\n",
      "551: 15.0%\n",
      "552: 20.0%\n",
      "553: 20.0%\n",
      "554: 15.0%\n",
      "555: 0.0%\n",
      "556: 40.0%\n",
      "557: 25.0%\n",
      "558: 20.0%\n",
      "559: 5.0%\n",
      "560: 10.0%\n",
      "561: 0.0%\n",
      "562: 10.0%\n",
      "563: 5.0%\n",
      "564: 20.0%\n",
      "565: 15.0%\n",
      "566: 5.0%\n",
      "567: 30.0%\n",
      "568: 30.0%\n",
      "569: 35.0%\n",
      "570: 10.0%\n",
      "571: 30.0%\n",
      "572: 15.0%\n",
      "573: 25.0%\n",
      "574: 5.0%\n",
      "575: 30.0%\n",
      "576: 15.0%\n",
      "577: 15.0%\n",
      "578: 20.0%\n",
      "579: 15.0%\n",
      "580: 20.0%\n",
      "581: 10.0%\n",
      "582: 0.0%\n",
      "583: 10.0%\n",
      "584: 15.0%\n",
      "585: 20.0%\n",
      "586: 25.0%\n",
      "587: 20.0%\n",
      "588: 5.0%\n",
      "589: 25.0%\n",
      "590: 45.0%\n",
      "591: 15.0%\n",
      "592: 15.0%\n",
      "593: 15.0%\n",
      "594: 20.0%\n",
      "595: 30.0%\n",
      "596: 25.0%\n",
      "597: 0.0%\n",
      "598: 10.0%\n",
      "599: 20.0%\n",
      "600: 20.0%\n",
      "601: 10.0%\n",
      "602: 15.0%\n",
      "603: 10.0%\n",
      "604: 10.0%\n",
      "605: 10.0%\n",
      "606: 25.0%\n",
      "607: 20.0%\n",
      "608: 25.0%\n",
      "609: 10.0%\n",
      "610: 25.0%\n",
      "611: 10.0%\n",
      "612: 20.0%\n",
      "613: 10.0%\n",
      "614: 10.0%\n",
      "615: 10.0%\n",
      "616: 20.0%\n",
      "617: 5.0%\n",
      "618: 20.0%\n",
      "619: 35.0%\n",
      "620: 15.0%\n",
      "621: 20.0%\n",
      "622: 20.0%\n",
      "623: 10.0%\n",
      "624: 15.0%\n",
      "625: 30.0%\n",
      "626: 25.0%\n",
      "627: 10.0%\n",
      "628: 5.0%\n",
      "629: 20.0%\n",
      "630: 0.0%\n",
      "631: 25.0%\n",
      "632: 25.0%\n",
      "633: 20.0%\n",
      "634: 20.0%\n",
      "635: 30.0%\n",
      "636: 25.0%\n",
      "637: 15.0%\n",
      "638: 25.0%\n",
      "639: 0.0%\n",
      "640: 15.0%\n",
      "641: 15.0%\n",
      "642: 15.0%\n",
      "643: 10.0%\n",
      "644: 5.0%\n",
      "645: 10.0%\n",
      "646: 5.0%\n",
      "647: 10.0%\n",
      "648: 25.0%\n",
      "649: 15.0%\n",
      "650: 10.0%\n",
      "651: 0.0%\n",
      "652: 20.0%\n",
      "653: 35.0%\n",
      "654: 15.0%\n",
      "655: 10.0%\n",
      "656: 20.0%\n",
      "657: 10.0%\n",
      "658: 5.0%\n",
      "659: 15.0%\n",
      "660: 40.0%\n",
      "661: 5.0%\n",
      "662: 25.0%\n",
      "663: 20.0%\n",
      "664: 15.0%\n",
      "665: 30.0%\n",
      "666: 15.0%\n",
      "667: 35.0%\n",
      "668: 30.0%\n",
      "669: 35.0%\n",
      "670: 20.0%\n",
      "671: 20.0%\n",
      "672: 20.0%\n",
      "673: 15.0%\n",
      "674: 25.0%\n",
      "675: 20.0%\n",
      "676: 5.0%\n",
      "677: 10.0%\n",
      "678: 10.0%\n",
      "679: 20.0%\n",
      "680: 15.0%\n",
      "681: 20.0%\n",
      "682: 15.0%\n",
      "683: 10.0%\n",
      "684: 0.0%\n",
      "685: 15.0%\n",
      "686: 15.0%\n",
      "687: 25.0%\n",
      "688: 25.0%\n",
      "689: 5.0%\n",
      "690: 15.0%\n",
      "691: 5.0%\n",
      "692: 15.0%\n",
      "693: 20.0%\n",
      "694: 15.0%\n",
      "695: 25.0%\n",
      "696: 20.0%\n",
      "697: 15.0%\n",
      "698: 20.0%\n",
      "699: 30.0%\n",
      "700: 35.0%\n",
      "701: 25.0%\n",
      "702: 15.0%\n",
      "703: 10.0%\n",
      "704: 5.0%\n",
      "705: 10.0%\n",
      "706: 10.0%\n",
      "707: 15.0%\n",
      "708: 10.0%\n",
      "709: 15.0%\n",
      "710: 15.0%\n",
      "711: 30.0%\n",
      "712: 30.0%\n",
      "713: 15.0%\n",
      "714: 25.0%\n",
      "715: 10.0%\n",
      "716: 10.0%\n",
      "717: 30.0%\n",
      "718: 15.0%\n",
      "719: 20.0%\n",
      "720: 10.0%\n",
      "721: 25.0%\n",
      "722: 25.0%\n",
      "723: 20.0%\n",
      "724: 10.0%\n",
      "725: 15.0%\n",
      "726: 10.0%\n",
      "727: 15.0%\n",
      "728: 40.0%\n",
      "729: 10.0%\n",
      "730: 15.0%\n",
      "731: 15.0%\n",
      "732: 10.0%\n",
      "733: 20.0%\n",
      "734: 10.0%\n",
      "735: 25.0%\n",
      "736: 10.0%\n",
      "737: 15.0%\n",
      "738: 15.0%\n",
      "739: 5.0%\n",
      "740: 25.0%\n",
      "741: 5.0%\n",
      "742: 20.0%\n",
      "743: 15.0%\n",
      "744: 15.0%\n",
      "745: 10.0%\n",
      "746: 20.0%\n",
      "747: 5.0%\n",
      "748: 10.0%\n",
      "749: 15.0%\n",
      "750: 10.0%\n",
      "751: 10.0%\n",
      "752: 20.0%\n",
      "753: 10.0%\n",
      "754: 5.0%\n",
      "755: 25.0%\n",
      "756: 10.0%\n",
      "757: 30.0%\n",
      "758: 25.0%\n",
      "759: 10.0%\n",
      "760: 5.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761: 25.0%\n",
      "762: 15.0%\n",
      "763: 20.0%\n",
      "764: 20.0%\n",
      "765: 15.0%\n",
      "766: 20.0%\n",
      "767: 5.0%\n",
      "768: 20.0%\n",
      "769: 15.0%\n",
      "770: 20.0%\n",
      "771: 25.0%\n",
      "772: 10.0%\n",
      "773: 20.0%\n",
      "774: 10.0%\n",
      "775: 15.0%\n",
      "776: 5.0%\n",
      "777: 15.0%\n",
      "778: 25.0%\n",
      "779: 15.0%\n",
      "780: 20.0%\n",
      "781: 15.0%\n",
      "782: 20.0%\n",
      "783: 15.0%\n",
      "784: 30.0%\n",
      "785: 35.0%\n",
      "786: 25.0%\n",
      "787: 20.0%\n",
      "788: 25.0%\n",
      "789: 5.0%\n",
      "790: 25.0%\n",
      "791: 10.0%\n",
      "792: 15.0%\n",
      "793: 15.0%\n",
      "794: 30.0%\n",
      "795: 10.0%\n",
      "796: 10.0%\n",
      "797: 5.0%\n",
      "798: 5.0%\n",
      "799: 20.0%\n",
      "800: 10.0%\n",
      "801: 5.0%\n",
      "802: 25.0%\n",
      "803: 10.0%\n",
      "804: 30.0%\n",
      "805: 25.0%\n",
      "806: 5.0%\n",
      "807: 25.0%\n",
      "808: 10.0%\n",
      "809: 15.0%\n",
      "810: 10.0%\n",
      "811: 20.0%\n",
      "812: 10.0%\n",
      "813: 10.0%\n",
      "814: 20.0%\n",
      "815: 10.0%\n",
      "816: 5.0%\n",
      "817: 10.0%\n",
      "818: 10.0%\n",
      "819: 30.0%\n",
      "820: 5.0%\n",
      "821: 10.0%\n",
      "822: 10.0%\n",
      "823: 10.0%\n",
      "824: 40.0%\n",
      "825: 15.0%\n",
      "826: 10.0%\n",
      "827: 20.0%\n",
      "828: 15.0%\n",
      "829: 20.0%\n",
      "830: 10.0%\n",
      "831: 15.0%\n",
      "832: 10.0%\n",
      "833: 10.0%\n",
      "834: 20.0%\n",
      "835: 10.0%\n",
      "836: 15.0%\n",
      "837: 45.0%\n",
      "838: 5.0%\n",
      "839: 5.0%\n",
      "840: 25.0%\n",
      "841: 30.0%\n",
      "842: 30.0%\n",
      "843: 5.0%\n",
      "844: 20.0%\n",
      "845: 5.0%\n",
      "846: 15.0%\n",
      "847: 15.0%\n",
      "848: 15.0%\n",
      "849: 10.0%\n",
      "850: 0.0%\n",
      "851: 20.0%\n",
      "852: 20.0%\n",
      "853: 25.0%\n",
      "854: 5.0%\n",
      "855: 25.0%\n",
      "856: 5.0%\n",
      "857: 0.0%\n",
      "858: 5.0%\n",
      "859: 20.0%\n",
      "860: 10.0%\n",
      "861: 40.0%\n",
      "862: 20.0%\n",
      "863: 35.0%\n",
      "864: 5.0%\n",
      "865: 5.0%\n",
      "866: 20.0%\n",
      "867: 10.0%\n",
      "868: 15.0%\n",
      "869: 10.0%\n",
      "870: 20.0%\n",
      "871: 20.0%\n",
      "872: 20.0%\n",
      "873: 15.0%\n",
      "874: 15.0%\n",
      "875: 15.0%\n",
      "876: 20.0%\n",
      "877: 15.0%\n",
      "878: 15.0%\n",
      "879: 20.0%\n",
      "880: 10.0%\n",
      "881: 20.0%\n",
      "882: 15.0%\n",
      "883: 10.0%\n",
      "884: 20.0%\n",
      "885: 15.0%\n",
      "886: 10.0%\n",
      "887: 15.0%\n",
      "888: 20.0%\n",
      "889: 20.0%\n",
      "890: 10.0%\n",
      "891: 10.0%\n",
      "892: 20.0%\n",
      "893: 15.0%\n",
      "894: 10.0%\n",
      "895: 15.0%\n",
      "896: 15.0%\n",
      "897: 10.0%\n",
      "898: 10.0%\n",
      "899: 20.0%\n",
      "900: 15.0%\n",
      "901: 30.0%\n",
      "902: 10.0%\n",
      "903: 15.0%\n",
      "904: 25.0%\n",
      "905: 10.0%\n",
      "906: 20.0%\n",
      "907: 5.0%\n",
      "908: 25.0%\n",
      "909: 25.0%\n",
      "910: 15.0%\n",
      "911: 30.0%\n",
      "912: 25.0%\n",
      "913: 25.0%\n",
      "914: 25.0%\n",
      "915: 15.0%\n",
      "916: 15.0%\n",
      "917: 25.0%\n",
      "918: 15.0%\n",
      "919: 5.0%\n",
      "920: 25.0%\n",
      "921: 20.0%\n",
      "922: 5.0%\n",
      "923: 10.0%\n",
      "924: 20.0%\n",
      "925: 10.0%\n",
      "926: 20.0%\n",
      "927: 30.0%\n",
      "928: 25.0%\n",
      "929: 10.0%\n",
      "930: 10.0%\n",
      "931: 0.0%\n",
      "932: 25.0%\n",
      "933: 5.0%\n",
      "934: 5.0%\n",
      "935: 15.0%\n",
      "936: 20.0%\n",
      "937: 20.0%\n",
      "938: 10.0%\n",
      "939: 15.0%\n",
      "940: 20.0%\n",
      "941: 10.0%\n",
      "942: 5.0%\n",
      "943: 15.0%\n",
      "944: 20.0%\n",
      "945: 20.0%\n",
      "946: 15.0%\n",
      "947: 10.0%\n",
      "948: 10.0%\n",
      "949: 10.0%\n",
      "950: 15.0%\n",
      "951: 25.0%\n",
      "952: 10.0%\n",
      "953: 25.0%\n",
      "954: 15.0%\n",
      "955: 30.0%\n",
      "956: 10.0%\n",
      "957: 15.0%\n",
      "958: 10.0%\n",
      "959: 30.0%\n",
      "960: 15.0%\n",
      "961: 15.0%\n",
      "962: 30.0%\n",
      "963: 5.0%\n",
      "964: 15.0%\n",
      "965: 15.0%\n",
      "966: 20.0%\n",
      "967: 20.0%\n",
      "968: 20.0%\n",
      "969: 0.0%\n",
      "970: 10.0%\n",
      "971: 10.0%\n",
      "972: 10.0%\n",
      "973: 20.0%\n",
      "974: 20.0%\n",
      "975: 20.0%\n",
      "976: 15.0%\n",
      "977: 15.0%\n",
      "978: 0.0%\n",
      "979: 10.0%\n",
      "980: 5.0%\n",
      "981: 20.0%\n",
      "982: 5.0%\n",
      "983: 10.0%\n",
      "984: 5.0%\n",
      "985: 25.0%\n",
      "986: 20.0%\n",
      "987: 25.0%\n",
      "988: 10.0%\n",
      "989: 15.0%\n",
      "990: 0.0%\n",
      "991: 15.0%\n",
      "992: 5.0%\n",
      "993: 25.0%\n",
      "994: 15.0%\n",
      "995: 5.0%\n",
      "996: 20.0%\n",
      "997: 5.0%\n",
      "998: 10.0%\n",
      "999: 5.0%\n",
      "1000: 25.0%\n",
      "1001: 15.0%\n",
      "1002: 10.0%\n",
      "1003: 25.0%\n",
      "1004: 35.0%\n",
      "1005: 10.0%\n",
      "1006: 20.0%\n",
      "1007: 15.0%\n",
      "1008: 10.0%\n",
      "1009: 10.0%\n",
      "1010: 10.0%\n",
      "1011: 5.0%\n",
      "1012: 15.0%\n",
      "1013: 5.0%\n",
      "1014: 10.0%\n",
      "1015: 10.0%\n",
      "1016: 15.0%\n",
      "1017: 30.0%\n",
      "1018: 10.0%\n",
      "1019: 15.0%\n",
      "1020: 15.0%\n",
      "1021: 25.0%\n",
      "1022: 20.0%\n",
      "1023: 30.0%\n",
      "1024: 5.0%\n",
      "1025: 15.0%\n",
      "1026: 25.0%\n",
      "1027: 10.0%\n",
      "1028: 15.0%\n",
      "1029: 15.0%\n",
      "1030: 5.0%\n",
      "1031: 5.0%\n",
      "1032: 5.0%\n",
      "1033: 10.0%\n",
      "1034: 20.0%\n",
      "1035: 0.0%\n",
      "1036: 25.0%\n",
      "1037: 10.0%\n",
      "1038: 5.0%\n",
      "1039: 15.0%\n",
      "1040: 15.0%\n",
      "1041: 20.0%\n",
      "1042: 5.0%\n",
      "1043: 10.0%\n",
      "1044: 15.0%\n",
      "1045: 10.0%\n",
      "1046: 15.0%\n",
      "1047: 15.0%\n",
      "1048: 5.0%\n",
      "1049: 10.0%\n",
      "1050: 25.0%\n",
      "1051: 15.0%\n",
      "1052: 15.0%\n",
      "1053: 25.0%\n",
      "1054: 20.0%\n",
      "1055: 15.0%\n",
      "1056: 0.0%\n",
      "1057: 20.0%\n",
      "1058: 15.0%\n",
      "1059: 30.0%\n",
      "1060: 10.0%\n",
      "1061: 10.0%\n",
      "1062: 10.0%\n",
      "1063: 15.0%\n",
      "1064: 10.0%\n",
      "1065: 5.0%\n",
      "1066: 20.0%\n",
      "1067: 25.0%\n",
      "1068: 10.0%\n",
      "1069: 0.0%\n",
      "1070: 15.0%\n",
      "1071: 5.0%\n",
      "1072: 0.0%\n",
      "1073: 15.0%\n",
      "1074: 5.0%\n",
      "1075: 20.0%\n",
      "1076: 20.0%\n",
      "1077: 15.0%\n",
      "1078: 5.0%\n",
      "1079: 15.0%\n",
      "1080: 15.0%\n",
      "1081: 25.0%\n",
      "1082: 35.0%\n",
      "1083: 10.0%\n",
      "1084: 20.0%\n",
      "1085: 20.0%\n",
      "1086: 25.0%\n",
      "1087: 10.0%\n",
      "1088: 20.0%\n",
      "1089: 10.0%\n",
      "1090: 10.0%\n",
      "1091: 15.0%\n",
      "1092: 10.0%\n",
      "1093: 30.0%\n",
      "1094: 25.0%\n",
      "1095: 20.0%\n",
      "1096: 25.0%\n",
      "1097: 15.0%\n",
      "1098: 5.0%\n",
      "1099: 5.0%\n",
      "1100: 0.0%\n",
      "1101: 5.0%\n",
      "1102: 15.0%\n",
      "1103: 15.0%\n",
      "1104: 25.0%\n",
      "1105: 10.0%\n",
      "1106: 35.0%\n",
      "1107: 0.0%\n",
      "1108: 15.0%\n",
      "1109: 5.0%\n",
      "1110: 15.0%\n",
      "1111: 15.0%\n",
      "1112: 5.0%\n",
      "1113: 15.0%\n",
      "1114: 0.0%\n",
      "1115: 20.0%\n",
      "1116: 10.0%\n",
      "1117: 5.0%\n",
      "1118: 10.0%\n",
      "1119: 10.0%\n",
      "1120: 10.0%\n",
      "1121: 25.0%\n",
      "1122: 20.0%\n",
      "1123: 20.0%\n",
      "1124: 25.0%\n",
      "1125: 15.0%\n",
      "1126: 5.0%\n",
      "1127: 10.0%\n",
      "1128: 10.0%\n",
      "1129: 0.0%\n",
      "1130: 0.0%\n",
      "1131: 20.0%\n",
      "1132: 20.0%\n",
      "1133: 10.0%\n",
      "1134: 20.0%\n",
      "1135: 5.0%\n",
      "1136: 10.0%\n",
      "1137: 15.0%\n",
      "1138: 10.0%\n",
      "1139: 5.0%\n",
      "1140: 20.0%\n",
      "1141: 20.0%\n",
      "1142: 15.0%\n",
      "1143: 25.0%\n",
      "1144: 0.0%\n",
      "1145: 30.0%\n",
      "1146: 20.0%\n",
      "1147: 10.0%\n",
      "1148: 15.0%\n",
      "1149: 15.0%\n",
      "1150: 10.0%\n",
      "1151: 5.0%\n",
      "1152: 20.0%\n",
      "1153: 10.0%\n",
      "1154: 30.0%\n",
      "1155: 5.0%\n",
      "1156: 20.0%\n",
      "1157: 15.0%\n",
      "1158: 10.0%\n",
      "1159: 10.0%\n",
      "1160: 10.0%\n",
      "1161: 15.0%\n",
      "1162: 10.0%\n",
      "1163: 20.0%\n",
      "1164: 15.0%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('graphs', sess.graph)\n",
    "    for index, batch in enumerate(batches):\n",
    "        feed = {model.data: batch[0], model.target: batch[1]}\n",
    "        error, _, summary_str = sess.run([model.error, model.optimize, model.summary], feed)\n",
    "        print('{}: {:3.1f}%'.format(index + 1, 100 * error))\n",
    "        if index % 1 == 0:\n",
    "            summary_writer.add_summary(summary_str, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
