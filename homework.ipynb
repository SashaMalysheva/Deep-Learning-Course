{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework:\n",
    "# Deep Convolutional Generative Adversarial Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of an implementation of DCGAN can be found in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the MNIST dataset. input_data is a library that downloads the dataset and uzips it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fashion-mnist/data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting fashion-mnist/data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting fashion-mnist/data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting fashion-mnist/data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('fashion-mnist/data/fashion', one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d_transpose(\\\n",
    "        zCon,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    \n",
    "    gen2 = slim.convolution2d_transpose(\\\n",
    "        gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    gen3 = slim.convolution2d_transpose(\\\n",
    "        gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    \n",
    "    g_out = slim.convolution2d_transpose(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 (13 points)\n",
    "Fill parameter for the discrimiator architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, reuse=False):\n",
    "    with slim.arg_scope([slim.fully_connected, slim.convolution2d], reuse=reuse, weights_initializer=initializer):\n",
    "        dis1 = slim.convolution2d(bottom, scope='d_conv1', num_outputs=16,\n",
    "                                  kernel_size=[5, 5], padding=\"SAME\", \n",
    "                                  normalizer_fn=slim.batch_norm, activation_fn=lrelu)\n",
    "\n",
    "        dis2 = slim.convolution2d(dis1, scope='d_conv2', num_outputs=32,\n",
    "                                  kernel_size=[5, 5], stride=[2, 2],\n",
    "                                  padding=\"SAME\", normalizer_fn=slim.batch_norm, \n",
    "                                  activation_fn=lrelu)\n",
    "\n",
    "        dis3 = slim.convolution2d(dis2, scope='d_conv3', num_outputs=64,\n",
    "                                  kernel_size=[5, 5], stride=[2, 2], \n",
    "                                  padding=\"SAME\", normalizer_fn=slim.batch_norm, \n",
    "                                  activation_fn=lrelu)\n",
    "        \n",
    "        dis4 = slim.convolution2d(dis3, scope='d_conv4', num_outputs=128,\n",
    "                                  kernel_size=[5, 5], stride=[2, 2], padding=\"SAME\",\n",
    "                                  normalizer_fn=slim.batch_norm, activation_fn=lrelu)\n",
    "        \n",
    "        d_out = slim.fully_connected(slim.flatten(dis4), scope='d_out', \n",
    "                                     num_outputs=1, activation_fn=tf.nn.sigmoid, \n",
    "                                     weights_initializer=initializer)\n",
    "                                     \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "\n",
    "Gz = generator(z_in) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "I strongly advise you to skip this cell and go the the next one since training will take you enormous amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf14ae8b321456c8ef0d8729e1ffd33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.564625 Disc Loss: 1.27914\n",
      "Gen Loss: 0.740345 Disc Loss: 1.99015\n",
      "Gen Loss: 1.24719 Disc Loss: 1.51429\n",
      "Gen Loss: 2.15557 Disc Loss: 0.789321\n",
      "Gen Loss: 1.6379 Disc Loss: 1.46493\n",
      "Gen Loss: 2.36797 Disc Loss: 0.974535\n",
      "Gen Loss: 2.12971 Disc Loss: 1.80872\n",
      "Gen Loss: 1.91726 Disc Loss: 1.58629\n",
      "Gen Loss: 1.59167 Disc Loss: 0.900938\n",
      "Gen Loss: 1.75992 Disc Loss: 1.02458\n",
      "Gen Loss: 1.47976 Disc Loss: 0.86475\n",
      "Gen Loss: 1.71782 Disc Loss: 0.718296\n",
      "Gen Loss: 2.13782 Disc Loss: 0.613305\n",
      "Gen Loss: 2.51764 Disc Loss: 0.798337\n",
      "Gen Loss: 1.65068 Disc Loss: 0.655746\n",
      "Gen Loss: 2.15268 Disc Loss: 0.529501\n",
      "Gen Loss: 1.36345 Disc Loss: 0.782065\n",
      "Gen Loss: 1.76479 Disc Loss: 0.587499\n",
      "Gen Loss: 3.44534 Disc Loss: 0.73005\n",
      "Gen Loss: 2.2702 Disc Loss: 0.604302\n",
      "Gen Loss: 3.35035 Disc Loss: 0.150696\n",
      "Gen Loss: 2.29616 Disc Loss: 0.392554\n",
      "Gen Loss: 3.34761 Disc Loss: 0.31526\n",
      "Gen Loss: 3.9101 Disc Loss: 0.0748567\n",
      "Gen Loss: 1.36194 Disc Loss: 0.257263\n",
      "Gen Loss: 2.59053 Disc Loss: 0.490398\n",
      "Gen Loss: 2.91366 Disc Loss: 0.272814\n",
      "Gen Loss: 2.40351 Disc Loss: 0.337206\n",
      "Gen Loss: 2.76872 Disc Loss: 0.230564\n",
      "Gen Loss: 3.12057 Disc Loss: 0.397499\n",
      "Gen Loss: 3.46544 Disc Loss: 0.388641\n",
      "Gen Loss: 2.76799 Disc Loss: 0.262768\n",
      "Gen Loss: 3.75759 Disc Loss: 0.37573\n",
      "Gen Loss: 3.01209 Disc Loss: 0.225354\n",
      "Gen Loss: 4.69828 Disc Loss: 0.408242\n",
      "Gen Loss: 6.32563 Disc Loss: 1.42603\n",
      "Gen Loss: 2.62068 Disc Loss: 0.290462\n",
      "Gen Loss: 2.94251 Disc Loss: 0.269415\n",
      "Gen Loss: 3.32947 Disc Loss: 0.106058\n",
      "Gen Loss: 5.95174 Disc Loss: 0.742793\n",
      "Gen Loss: 3.42715 Disc Loss: 0.188861\n",
      "Gen Loss: 3.2178 Disc Loss: 0.224217\n",
      "Gen Loss: 2.91779 Disc Loss: 0.0935387\n",
      "Gen Loss: 4.31208 Disc Loss: 0.0613146\n",
      "Gen Loss: 3.85201 Disc Loss: 0.5122\n",
      "Gen Loss: 3.82325 Disc Loss: 0.0884317\n",
      "Gen Loss: 3.59571 Disc Loss: 0.125772\n",
      "Gen Loss: 3.19226 Disc Loss: 0.253606\n",
      "Gen Loss: 3.15365 Disc Loss: 0.0896095\n",
      "Gen Loss: 4.302 Disc Loss: 0.251536\n",
      "Gen Loss: 3.53263 Disc Loss: 0.076308\n",
      "Gen Loss: 3.46199 Disc Loss: 0.119255\n",
      "Gen Loss: 4.31677 Disc Loss: 0.0739849\n",
      "Gen Loss: 3.6341 Disc Loss: 0.0696141\n",
      "Gen Loss: 3.95954 Disc Loss: 0.113465\n",
      "Gen Loss: 4.24854 Disc Loss: 0.0427749\n",
      "Gen Loss: 4.2048 Disc Loss: 0.0900494\n",
      "Gen Loss: 3.26831 Disc Loss: 0.204759\n",
      "Gen Loss: 6.14683 Disc Loss: 0.320186\n",
      "Gen Loss: 2.25988 Disc Loss: 0.260274\n",
      "Gen Loss: 3.34078 Disc Loss: 0.255454\n",
      "Gen Loss: 3.78294 Disc Loss: 0.0788217\n",
      "Gen Loss: 4.35349 Disc Loss: 0.0519696\n",
      "Gen Loss: 3.59392 Disc Loss: 0.0719787\n",
      "Gen Loss: 7.11986 Disc Loss: 0.859971\n",
      "Gen Loss: 4.25983 Disc Loss: 0.0964428\n",
      "Gen Loss: 2.47969 Disc Loss: 0.819943\n",
      "Gen Loss: 2.2868 Disc Loss: 0.170468\n",
      "Gen Loss: 0.363226 Disc Loss: 0.448314\n",
      "Gen Loss: 3.39736 Disc Loss: 0.18651\n",
      "Gen Loss: 3.14017 Disc Loss: 0.102439\n",
      "Gen Loss: 5.53015 Disc Loss: 0.772024\n",
      "Gen Loss: 3.29202 Disc Loss: 0.137875\n",
      "Gen Loss: 3.27768 Disc Loss: 0.29646\n",
      "Gen Loss: 3.74246 Disc Loss: 0.152455\n",
      "Gen Loss: 2.47368 Disc Loss: 0.0764087\n",
      "Gen Loss: 6.48203 Disc Loss: 0.705263\n",
      "Gen Loss: 6.36149 Disc Loss: 0.504991\n",
      "Gen Loss: 2.64666 Disc Loss: 0.536532\n",
      "Gen Loss: 2.6811 Disc Loss: 0.185855\n",
      "Gen Loss: 2.78099 Disc Loss: 0.217549\n",
      "Gen Loss: 3.36505 Disc Loss: 0.187157\n",
      "Gen Loss: 3.14028 Disc Loss: 0.117831\n",
      "Gen Loss: 3.56411 Disc Loss: 0.120946\n",
      "Gen Loss: 1.09112 Disc Loss: 0.328102\n",
      "Gen Loss: 6.16667 Disc Loss: 0.304479\n",
      "Gen Loss: 2.2078 Disc Loss: 0.322297\n",
      "Gen Loss: 4.28585 Disc Loss: 0.195239\n",
      "Gen Loss: 3.37777 Disc Loss: 0.115309\n",
      "Gen Loss: 3.59386 Disc Loss: 0.167782\n",
      "Gen Loss: 5.1223 Disc Loss: 1.98424\n",
      "Gen Loss: 1.6933 Disc Loss: 0.218835\n",
      "Gen Loss: 2.21969 Disc Loss: 0.428504\n",
      "Gen Loss: 2.02309 Disc Loss: 0.269754\n",
      "Gen Loss: 2.93434 Disc Loss: 0.209954\n",
      "Gen Loss: 3.06996 Disc Loss: 0.242155\n",
      "Gen Loss: 6.45232 Disc Loss: 0.396683\n",
      "Gen Loss: 2.8112 Disc Loss: 0.0506839\n",
      "Gen Loss: 5.27928 Disc Loss: 0.0531484\n",
      "Gen Loss: 4.64884 Disc Loss: 0.0296446\n",
      "Gen Loss: 3.83508 Disc Loss: 0.0349033\n",
      "Saved Model\n",
      "Gen Loss: 5.94775 Disc Loss: 0.660257\n",
      "Gen Loss: 4.32685 Disc Loss: 0.176561\n",
      "Gen Loss: 2.88095 Disc Loss: 0.123937\n",
      "Gen Loss: 1.59974 Disc Loss: 0.231755\n",
      "Gen Loss: 7.66829 Disc Loss: 0.883313\n",
      "Gen Loss: 3.54644 Disc Loss: 0.894081\n",
      "Gen Loss: 1.86749 Disc Loss: 0.464831\n",
      "Gen Loss: 3.66454 Disc Loss: 0.151979\n",
      "Gen Loss: 4.10146 Disc Loss: 0.329035\n",
      "Gen Loss: 3.16436 Disc Loss: 0.147894\n",
      "Gen Loss: 4.14537 Disc Loss: 0.0940528\n",
      "Gen Loss: 2.92847 Disc Loss: 0.172172\n",
      "Gen Loss: 7.32588 Disc Loss: 0.679521\n",
      "Gen Loss: 3.69864 Disc Loss: 0.529916\n",
      "Gen Loss: 2.90289 Disc Loss: 0.0919471\n",
      "Gen Loss: 5.85285 Disc Loss: 0.381672\n",
      "Gen Loss: 2.61472 Disc Loss: 0.267751\n",
      "Gen Loss: 1.61319 Disc Loss: 0.27539\n",
      "Gen Loss: 4.95468 Disc Loss: 0.0859841\n",
      "Gen Loss: 3.99056 Disc Loss: 0.239849\n",
      "Gen Loss: 2.72996 Disc Loss: 0.191297\n",
      "Gen Loss: 1.51134 Disc Loss: 0.343576\n",
      "Gen Loss: 1.65636 Disc Loss: 0.70386\n",
      "Gen Loss: 3.80624 Disc Loss: 0.501856\n",
      "Gen Loss: 2.84394 Disc Loss: 0.301512\n",
      "Gen Loss: 1.1611 Disc Loss: 0.503057\n",
      "Gen Loss: 2.65013 Disc Loss: 0.642619\n",
      "Gen Loss: 2.88334 Disc Loss: 0.610369\n",
      "Gen Loss: 1.14736 Disc Loss: 0.995584\n",
      "Gen Loss: 2.23534 Disc Loss: 0.674584\n",
      "Gen Loss: 1.98101 Disc Loss: 0.577789\n",
      "Gen Loss: 2.99706 Disc Loss: 0.315324\n",
      "Gen Loss: 1.78856 Disc Loss: 0.382563\n",
      "Gen Loss: 5.16501 Disc Loss: 1.36453\n",
      "Gen Loss: 3.33505 Disc Loss: 0.658529\n",
      "Gen Loss: 4.10502 Disc Loss: 0.729885\n",
      "Gen Loss: 2.92799 Disc Loss: 0.282398\n",
      "Gen Loss: 2.54556 Disc Loss: 0.369682\n",
      "Gen Loss: 0.897588 Disc Loss: 0.481015\n",
      "Gen Loss: 1.73722 Disc Loss: 0.412864\n",
      "Gen Loss: 2.27535 Disc Loss: 0.598541\n",
      "Gen Loss: 1.68768 Disc Loss: 0.590896\n",
      "Gen Loss: 3.25803 Disc Loss: 0.365237\n",
      "Gen Loss: 5.68034 Disc Loss: 1.61002\n",
      "Gen Loss: 1.59049 Disc Loss: 1.07481\n",
      "Gen Loss: 2.07462 Disc Loss: 0.605368\n",
      "Gen Loss: 3.76809 Disc Loss: 0.833523\n",
      "Gen Loss: 1.93004 Disc Loss: 0.612515\n",
      "Gen Loss: 2.76404 Disc Loss: 0.749403\n",
      "Gen Loss: 2.00163 Disc Loss: 0.620429\n",
      "Gen Loss: 2.45188 Disc Loss: 0.224941\n",
      "Gen Loss: 2.50114 Disc Loss: 0.766394\n",
      "Gen Loss: 2.29106 Disc Loss: 0.403373\n",
      "Gen Loss: 2.77958 Disc Loss: 0.240788\n",
      "Gen Loss: 1.96566 Disc Loss: 0.377985\n",
      "Gen Loss: 2.58734 Disc Loss: 0.242537\n",
      "Gen Loss: 2.24903 Disc Loss: 0.3188\n",
      "Gen Loss: 2.86629 Disc Loss: 0.234327\n",
      "Gen Loss: 3.59024 Disc Loss: 0.477724\n",
      "Gen Loss: 1.34073 Disc Loss: 0.531774\n",
      "Gen Loss: 2.37575 Disc Loss: 0.258597\n",
      "Gen Loss: 4.90341 Disc Loss: 0.56076\n",
      "Gen Loss: 2.70224 Disc Loss: 0.481155\n",
      "Gen Loss: 5.7631 Disc Loss: 1.01505\n",
      "Gen Loss: 1.84939 Disc Loss: 0.612398\n",
      "Gen Loss: 2.93899 Disc Loss: 1.04955\n",
      "Gen Loss: 3.80076 Disc Loss: 0.798779\n",
      "Gen Loss: 1.91411 Disc Loss: 0.758523\n",
      "Gen Loss: 2.06185 Disc Loss: 0.717437\n",
      "Gen Loss: 3.12221 Disc Loss: 0.81706\n",
      "Gen Loss: 2.1746 Disc Loss: 0.524668\n",
      "Gen Loss: 2.90529 Disc Loss: 0.44721\n",
      "Gen Loss: 2.39237 Disc Loss: 0.323854\n",
      "Gen Loss: 2.72156 Disc Loss: 0.342801\n",
      "Gen Loss: 1.58278 Disc Loss: 0.623297\n",
      "Gen Loss: 0.952765 Disc Loss: 0.860464\n",
      "Gen Loss: 2.24321 Disc Loss: 0.203841\n",
      "Gen Loss: 1.98871 Disc Loss: 0.379914\n",
      "Gen Loss: 2.13956 Disc Loss: 0.262053\n",
      "Gen Loss: 5.81794 Disc Loss: 1.59288\n",
      "Gen Loss: 2.90826 Disc Loss: 0.535386\n",
      "Gen Loss: 2.77557 Disc Loss: 0.424414\n",
      "Gen Loss: 1.33956 Disc Loss: 0.572521\n",
      "Gen Loss: 2.78012 Disc Loss: 0.676891\n",
      "Gen Loss: 2.93818 Disc Loss: 0.347851\n",
      "Gen Loss: 1.59414 Disc Loss: 0.702909\n",
      "Gen Loss: 3.99543 Disc Loss: 0.836591\n",
      "Gen Loss: 2.02001 Disc Loss: 0.589489\n",
      "Gen Loss: 1.06291 Disc Loss: 0.7349\n",
      "Gen Loss: 3.84091 Disc Loss: 0.634977\n",
      "Gen Loss: 2.10917 Disc Loss: 0.538745\n",
      "Gen Loss: 1.99429 Disc Loss: 0.839418\n",
      "Gen Loss: 1.76229 Disc Loss: 0.478265\n",
      "Gen Loss: 1.57964 Disc Loss: 0.528406\n",
      "Gen Loss: 2.31835 Disc Loss: 0.35319\n",
      "Gen Loss: 3.02582 Disc Loss: 0.199634\n",
      "Gen Loss: 4.49924 Disc Loss: 0.96069\n",
      "Gen Loss: 2.26712 Disc Loss: 0.351915\n",
      "Gen Loss: 3.22257 Disc Loss: 0.765378\n",
      "Gen Loss: 1.10425 Disc Loss: 0.981088\n",
      "Saved Model\n",
      "Gen Loss: 2.47553 Disc Loss: 0.928805\n",
      "Gen Loss: 2.1551 Disc Loss: 0.473737\n",
      "Gen Loss: 2.74713 Disc Loss: 0.304565\n",
      "Gen Loss: 3.13761 Disc Loss: 0.414066\n",
      "Gen Loss: 1.93973 Disc Loss: 0.390375\n",
      "Gen Loss: 2.88397 Disc Loss: 0.329141\n",
      "Gen Loss: 3.81446 Disc Loss: 0.56548\n",
      "Gen Loss: 3.83574 Disc Loss: 0.841727\n",
      "Gen Loss: 0.893936 Disc Loss: 0.555429\n",
      "Gen Loss: 3.06508 Disc Loss: 0.849083\n",
      "Gen Loss: 2.02396 Disc Loss: 0.646605\n",
      "Gen Loss: 3.66904 Disc Loss: 0.347765\n",
      "Gen Loss: 2.18748 Disc Loss: 0.384055\n",
      "Gen Loss: 1.893 Disc Loss: 0.409154\n",
      "Gen Loss: 0.891008 Disc Loss: 1.01535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 1.92051 Disc Loss: 0.331426\n",
      "Gen Loss: 1.69991 Disc Loss: 0.308055\n",
      "Gen Loss: 0.782882 Disc Loss: 1.50221\n",
      "Gen Loss: 2.72562 Disc Loss: 0.494593\n",
      "Gen Loss: 2.75386 Disc Loss: 0.220385\n",
      "Gen Loss: 3.01073 Disc Loss: 0.489476\n",
      "Gen Loss: 2.13449 Disc Loss: 0.344153\n",
      "Gen Loss: 2.72425 Disc Loss: 0.477243\n",
      "Gen Loss: 2.3309 Disc Loss: 0.341873\n",
      "Gen Loss: 2.66523 Disc Loss: 0.256782\n",
      "Gen Loss: 1.58822 Disc Loss: 0.835049\n",
      "Gen Loss: 0.76848 Disc Loss: 0.238643\n",
      "Gen Loss: 3.40578 Disc Loss: 0.467231\n",
      "Gen Loss: 2.27835 Disc Loss: 0.368069\n",
      "Gen Loss: 5.90252 Disc Loss: 0.69888\n",
      "Gen Loss: 1.27227 Disc Loss: 0.74139\n",
      "Gen Loss: 1.95852 Disc Loss: 0.518093\n",
      "Gen Loss: 2.1907 Disc Loss: 0.520672\n",
      "Gen Loss: 2.66277 Disc Loss: 0.351349\n",
      "Gen Loss: 1.374 Disc Loss: 0.489211\n",
      "Gen Loss: 2.15259 Disc Loss: 0.544119\n",
      "Gen Loss: 2.13021 Disc Loss: 0.315722\n",
      "Gen Loss: 2.4006 Disc Loss: 0.35948\n",
      "Gen Loss: 2.67619 Disc Loss: 0.318424\n",
      "Gen Loss: 2.30511 Disc Loss: 0.486682\n",
      "Gen Loss: 3.22856 Disc Loss: 0.260125\n",
      "Gen Loss: 7.66428 Disc Loss: 0.729131\n",
      "Gen Loss: 2.00575 Disc Loss: 0.465082\n",
      "Gen Loss: 2.2479 Disc Loss: 0.342112\n",
      "Gen Loss: 5.68059 Disc Loss: 0.637761\n",
      "Gen Loss: 5.42985 Disc Loss: 0.409346\n",
      "Gen Loss: 0.638623 Disc Loss: 1.08613\n",
      "Gen Loss: 2.40495 Disc Loss: 0.774874\n",
      "Gen Loss: 2.31667 Disc Loss: 0.367551\n",
      "Gen Loss: 2.94047 Disc Loss: 0.387129\n",
      "Gen Loss: 1.19258 Disc Loss: 0.780053\n",
      "Gen Loss: 2.31136 Disc Loss: 0.275204\n",
      "Gen Loss: 3.12161 Disc Loss: 0.307451\n",
      "Gen Loss: 5.09368 Disc Loss: 0.551689\n",
      "Gen Loss: 2.73628 Disc Loss: 0.49608\n",
      "Gen Loss: 3.87489 Disc Loss: 0.514854\n",
      "Gen Loss: 2.29387 Disc Loss: 0.269442\n",
      "Gen Loss: 2.49171 Disc Loss: 0.328377\n",
      "Gen Loss: 3.31734 Disc Loss: 0.595757\n",
      "Gen Loss: 2.91577 Disc Loss: 0.301366\n",
      "Gen Loss: 2.19252 Disc Loss: 0.318443\n",
      "Gen Loss: 0.415795 Disc Loss: 1.17638\n",
      "Gen Loss: 3.74656 Disc Loss: 0.527474\n",
      "Gen Loss: 1.2176 Disc Loss: 0.544683\n",
      "Gen Loss: 1.76798 Disc Loss: 0.550615\n",
      "Gen Loss: 2.50754 Disc Loss: 0.45067\n",
      "Gen Loss: 3.25463 Disc Loss: 0.130943\n",
      "Gen Loss: 3.0969 Disc Loss: 0.193678\n",
      "Gen Loss: 2.18751 Disc Loss: 0.958728\n",
      "Gen Loss: 2.39513 Disc Loss: 0.336314\n",
      "Gen Loss: 2.11217 Disc Loss: 0.232855\n",
      "Gen Loss: 1.66066 Disc Loss: 0.312518\n",
      "Gen Loss: 2.47743 Disc Loss: 0.189021\n",
      "Gen Loss: 1.79268 Disc Loss: 2.30551\n",
      "Gen Loss: 5.43733 Disc Loss: 0.886836\n",
      "Gen Loss: 3.20021 Disc Loss: 0.688636\n",
      "Gen Loss: 3.2389 Disc Loss: 0.248004\n",
      "Gen Loss: 2.4773 Disc Loss: 0.196798\n",
      "Gen Loss: 2.6371 Disc Loss: 0.368679\n",
      "Gen Loss: 2.2403 Disc Loss: 0.245555\n",
      "Gen Loss: 3.21046 Disc Loss: 0.255148\n",
      "Gen Loss: 6.9317 Disc Loss: 0.960751\n",
      "Gen Loss: 1.37351 Disc Loss: 0.304731\n",
      "Gen Loss: 2.58536 Disc Loss: 0.25879\n",
      "Gen Loss: 3.22942 Disc Loss: 0.379823\n",
      "Gen Loss: 2.79021 Disc Loss: 0.903817\n",
      "Gen Loss: 5.41364 Disc Loss: 0.649869\n",
      "Gen Loss: 2.49421 Disc Loss: 0.436939\n",
      "Gen Loss: 3.30345 Disc Loss: 0.192175\n",
      "Gen Loss: 4.19891 Disc Loss: 0.450343\n",
      "Gen Loss: 2.87038 Disc Loss: 0.23641\n",
      "Gen Loss: 2.21269 Disc Loss: 0.428755\n",
      "Gen Loss: 3.94221 Disc Loss: 0.52153\n",
      "Gen Loss: 2.41693 Disc Loss: 0.28609\n",
      "Gen Loss: 2.01933 Disc Loss: 0.221958\n",
      "Gen Loss: 2.76385 Disc Loss: 0.240988\n",
      "Gen Loss: 3.17565 Disc Loss: 0.445247\n",
      "Gen Loss: 3.52404 Disc Loss: 0.40944\n",
      "Gen Loss: 2.11405 Disc Loss: 0.315746\n",
      "Gen Loss: 2.75356 Disc Loss: 0.213334\n",
      "Saved Model\n",
      "Gen Loss: 4.44671 Disc Loss: 0.41077\n",
      "Gen Loss: 2.22839 Disc Loss: 0.229146\n",
      "Gen Loss: 3.64587 Disc Loss: 0.219684\n",
      "Gen Loss: 3.26277 Disc Loss: 0.195541\n",
      "Gen Loss: 2.20428 Disc Loss: 0.249041\n",
      "Gen Loss: 1.56878 Disc Loss: 0.273809\n",
      "Gen Loss: 5.17759 Disc Loss: 1.0754\n",
      "Gen Loss: 2.23155 Disc Loss: 0.411897\n",
      "Gen Loss: 2.58554 Disc Loss: 0.399832\n",
      "Gen Loss: 2.25206 Disc Loss: 0.200669\n",
      "Gen Loss: 1.49897 Disc Loss: 0.241253\n",
      "Gen Loss: 1.16267 Disc Loss: 0.178602\n",
      "Gen Loss: 3.2099 Disc Loss: 0.234742\n",
      "Gen Loss: 2.31409 Disc Loss: 0.271372\n",
      "Gen Loss: 2.60329 Disc Loss: 0.191953\n",
      "Gen Loss: 1.87617 Disc Loss: 0.359144\n",
      "Gen Loss: 2.86971 Disc Loss: 0.332368\n",
      "Gen Loss: 2.37391 Disc Loss: 0.375597\n",
      "Gen Loss: 1.63886 Disc Loss: 0.443478\n",
      "Gen Loss: 1.71256 Disc Loss: 0.456405\n",
      "Gen Loss: 2.26489 Disc Loss: 0.356704\n",
      "Gen Loss: 3.60058 Disc Loss: 0.24368\n",
      "Gen Loss: 2.38967 Disc Loss: 0.62617\n",
      "Gen Loss: 2.71471 Disc Loss: 0.187212\n",
      "Gen Loss: 3.07209 Disc Loss: 0.266855\n",
      "Gen Loss: 2.79455 Disc Loss: 0.235408\n",
      "Gen Loss: 3.98488 Disc Loss: 0.230263\n",
      "Gen Loss: 3.01206 Disc Loss: 0.377623\n",
      "Gen Loss: 6.20635 Disc Loss: 0.782787\n",
      "Gen Loss: 2.88797 Disc Loss: 0.291655\n",
      "Gen Loss: 3.21762 Disc Loss: 0.188656\n",
      "Gen Loss: 3.49468 Disc Loss: 0.179293\n",
      "Gen Loss: 2.68469 Disc Loss: 0.208841\n",
      "Gen Loss: 1.70055 Disc Loss: 0.241102\n",
      "Gen Loss: 3.30905 Disc Loss: 0.236611\n",
      "Gen Loss: 2.07435 Disc Loss: 0.315608\n",
      "Gen Loss: 3.41677 Disc Loss: 0.263288\n",
      "Gen Loss: 4.53595 Disc Loss: 0.607043\n",
      "Gen Loss: 2.53141 Disc Loss: 0.306947\n",
      "Gen Loss: 2.07896 Disc Loss: 0.395099\n",
      "Gen Loss: 4.51071 Disc Loss: 0.490906\n",
      "Gen Loss: 1.19403 Disc Loss: 0.608609\n",
      "Gen Loss: 1.50749 Disc Loss: 0.454934\n",
      "Gen Loss: 2.50522 Disc Loss: 0.296197\n",
      "Gen Loss: 2.30436 Disc Loss: 0.235659\n",
      "Gen Loss: 3.3102 Disc Loss: 0.680162\n",
      "Gen Loss: 3.36594 Disc Loss: 0.284067\n",
      "Gen Loss: 1.96679 Disc Loss: 0.143875\n",
      "Gen Loss: 2.6063 Disc Loss: 0.294781\n",
      "Gen Loss: 4.41878 Disc Loss: 0.475473\n",
      "Gen Loss: 3.14486 Disc Loss: 0.278119\n",
      "Gen Loss: 2.22635 Disc Loss: 0.298919\n",
      "Gen Loss: 2.75869 Disc Loss: 0.0981315\n",
      "Gen Loss: 5.68322 Disc Loss: 0.512403\n",
      "Gen Loss: 3.09776 Disc Loss: 0.30241\n",
      "Gen Loss: 2.51115 Disc Loss: 0.474333\n",
      "Gen Loss: 4.35676 Disc Loss: 0.234106\n",
      "Gen Loss: 4.47375 Disc Loss: 0.0750269\n",
      "Gen Loss: 2.93799 Disc Loss: 0.123939\n",
      "Gen Loss: 3.81555 Disc Loss: 0.267602\n",
      "Gen Loss: 3.64582 Disc Loss: 0.139038\n",
      "Gen Loss: 3.00803 Disc Loss: 0.109794\n",
      "Gen Loss: 3.21 Disc Loss: 0.378993\n",
      "Gen Loss: 1.01424 Disc Loss: 0.896184\n",
      "Gen Loss: 3.19656 Disc Loss: 0.310244\n",
      "Gen Loss: 4.37082 Disc Loss: 0.234773\n",
      "Gen Loss: 3.67111 Disc Loss: 0.168423\n",
      "Gen Loss: 2.47702 Disc Loss: 2.67727\n",
      "Gen Loss: 2.07462 Disc Loss: 0.374552\n",
      "Gen Loss: 2.8606 Disc Loss: 0.223459\n",
      "Gen Loss: 3.19694 Disc Loss: 0.129368\n",
      "Gen Loss: 3.14803 Disc Loss: 0.120161\n",
      "Gen Loss: 2.87821 Disc Loss: 0.10667\n",
      "Gen Loss: 2.68569 Disc Loss: 0.276919\n",
      "Gen Loss: 3.2038 Disc Loss: 0.222739\n",
      "Gen Loss: 3.32475 Disc Loss: 0.213927\n",
      "Gen Loss: 2.4088 Disc Loss: 0.182546\n",
      "Gen Loss: 2.51009 Disc Loss: 0.186584\n",
      "Gen Loss: 4.65748 Disc Loss: 0.392381\n",
      "Gen Loss: 6.74435 Disc Loss: 0.201734\n",
      "Gen Loss: 3.22871 Disc Loss: 0.664293\n",
      "Gen Loss: 2.59008 Disc Loss: 0.290551\n",
      "Gen Loss: 3.5213 Disc Loss: 0.234847\n",
      "Gen Loss: 3.67255 Disc Loss: 0.233499\n",
      "Gen Loss: 4.19286 Disc Loss: 0.333635\n",
      "Gen Loss: 2.61093 Disc Loss: 0.215284\n",
      "Gen Loss: 2.4565 Disc Loss: 0.280242\n",
      "Gen Loss: 5.02333 Disc Loss: 0.481764\n",
      "Gen Loss: 6.839 Disc Loss: 1.85394\n",
      "Gen Loss: 3.89176 Disc Loss: 0.364269\n",
      "Gen Loss: 3.45026 Disc Loss: 0.232201\n",
      "Gen Loss: 2.86798 Disc Loss: 0.285792\n",
      "Gen Loss: 1.74463 Disc Loss: 0.236029\n",
      "Gen Loss: 1.84405 Disc Loss: 0.275628\n",
      "Gen Loss: 2.76749 Disc Loss: 0.197742\n",
      "Gen Loss: 2.84571 Disc Loss: 0.128459\n",
      "Gen Loss: 2.34849 Disc Loss: 0.404062\n",
      "Gen Loss: 1.37481 Disc Loss: 0.362963\n",
      "Gen Loss: 3.80147 Disc Loss: 0.249249\n",
      "Gen Loss: 5.08153 Disc Loss: 0.177911\n",
      "Saved Model\n",
      "Gen Loss: 4.68871 Disc Loss: 0.269957\n",
      "Gen Loss: 3.95317 Disc Loss: 0.343193\n",
      "Gen Loss: 5.43082 Disc Loss: 0.792091\n",
      "Gen Loss: 3.79291 Disc Loss: 0.234546\n",
      "Gen Loss: 2.62456 Disc Loss: 0.340272\n",
      "Gen Loss: 8.83623 Disc Loss: 1.43464\n",
      "Gen Loss: 1.24094 Disc Loss: 0.518886\n",
      "Gen Loss: 2.65929 Disc Loss: 0.242078\n",
      "Gen Loss: 3.56348 Disc Loss: 0.133664\n",
      "Gen Loss: 1.30911 Disc Loss: 0.212716\n",
      "Gen Loss: 1.6556 Disc Loss: 0.308284\n",
      "Gen Loss: 5.49215 Disc Loss: 0.197318\n",
      "Gen Loss: 4.70859 Disc Loss: 0.298558\n",
      "Gen Loss: 2.76909 Disc Loss: 0.113949\n",
      "Gen Loss: 4.47166 Disc Loss: 0.161987\n",
      "Gen Loss: 1.71688 Disc Loss: 0.214287\n",
      "Gen Loss: 2.99247 Disc Loss: 0.244102\n",
      "Gen Loss: 3.00565 Disc Loss: 0.139084\n",
      "Gen Loss: 3.2518 Disc Loss: 0.158646\n",
      "Gen Loss: 3.36882 Disc Loss: 0.129154\n",
      "Gen Loss: 3.17289 Disc Loss: 0.124317\n",
      "Gen Loss: 2.26967 Disc Loss: 0.134574\n",
      "Gen Loss: 2.35402 Disc Loss: 0.239374\n",
      "Gen Loss: 3.58416 Disc Loss: 0.114403\n",
      "Gen Loss: 7.67922 Disc Loss: 1.04801\n",
      "Gen Loss: 2.39272 Disc Loss: 0.266261\n",
      "Gen Loss: 3.4517 Disc Loss: 0.267814\n",
      "Gen Loss: 1.95577 Disc Loss: 0.39771\n",
      "Gen Loss: 1.54708 Disc Loss: 0.559778\n",
      "Gen Loss: 7.0356 Disc Loss: 0.533603\n",
      "Gen Loss: 2.34491 Disc Loss: 0.20046\n",
      "Gen Loss: 6.3275 Disc Loss: 0.593417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 2.56561 Disc Loss: 0.183536\n",
      "Gen Loss: 3.35017 Disc Loss: 0.265374\n",
      "Gen Loss: 2.87639 Disc Loss: 0.214139\n",
      "Gen Loss: 4.22725 Disc Loss: 0.221296\n",
      "Gen Loss: 5.20471 Disc Loss: 0.297145\n",
      "Gen Loss: 3.41635 Disc Loss: 0.123128\n",
      "Gen Loss: 3.72994 Disc Loss: 0.0669656\n",
      "Gen Loss: 3.07744 Disc Loss: 0.175666\n",
      "Gen Loss: 3.17382 Disc Loss: 0.0993553\n",
      "Gen Loss: 2.07679 Disc Loss: 0.254548\n",
      "Gen Loss: 5.37261 Disc Loss: 0.356317\n",
      "Gen Loss: 3.50362 Disc Loss: 0.164372\n",
      "Gen Loss: 3.86144 Disc Loss: 0.236815\n",
      "Gen Loss: 1.50939 Disc Loss: 0.248538\n",
      "Gen Loss: 3.13616 Disc Loss: 0.221507\n",
      "Gen Loss: 2.24541 Disc Loss: 0.463886\n",
      "Gen Loss: 3.14606 Disc Loss: 0.284452\n",
      "Gen Loss: 1.17139 Disc Loss: 0.0819349\n",
      "Gen Loss: 5.19064 Disc Loss: 0.384261\n",
      "Gen Loss: 3.45809 Disc Loss: 0.226095\n",
      "Gen Loss: 2.7838 Disc Loss: 0.283597\n",
      "Gen Loss: 2.76624 Disc Loss: 0.299458\n",
      "Gen Loss: 3.10337 Disc Loss: 0.140346\n",
      "Gen Loss: 2.79304 Disc Loss: 0.159188\n",
      "Gen Loss: 2.79836 Disc Loss: 0.142834\n",
      "Gen Loss: 3.3819 Disc Loss: 0.170751\n",
      "Gen Loss: 1.61405 Disc Loss: 0.415707\n",
      "Gen Loss: 1.69791 Disc Loss: 0.351795\n",
      "Gen Loss: 2.11795 Disc Loss: 0.31759\n",
      "Gen Loss: 3.95744 Disc Loss: 0.186269\n",
      "Gen Loss: 2.77582 Disc Loss: 0.180793\n",
      "Gen Loss: 3.01873 Disc Loss: 0.142364\n",
      "Gen Loss: 2.56228 Disc Loss: 0.219276\n",
      "Gen Loss: 3.54056 Disc Loss: 0.105867\n",
      "Gen Loss: 3.67788 Disc Loss: 0.118739\n",
      "Gen Loss: 3.5557 Disc Loss: 0.169529\n",
      "Gen Loss: 4.53701 Disc Loss: 0.309116\n",
      "Gen Loss: 3.15946 Disc Loss: 0.166307\n",
      "Gen Loss: 3.28961 Disc Loss: 0.146399\n",
      "Gen Loss: 1.18059 Disc Loss: 0.29838\n",
      "Gen Loss: 4.2159 Disc Loss: 0.33372\n",
      "Gen Loss: 2.31253 Disc Loss: 0.244363\n",
      "Gen Loss: 3.19207 Disc Loss: 0.177013\n",
      "Gen Loss: 3.51157 Disc Loss: 0.213619\n",
      "Gen Loss: 5.32769 Disc Loss: 0.504232\n",
      "Gen Loss: 1.93372 Disc Loss: 0.332001\n",
      "Gen Loss: 2.52139 Disc Loss: 0.315339\n",
      "Gen Loss: 2.45878 Disc Loss: 0.17498\n",
      "Gen Loss: 3.14821 Disc Loss: 0.277023\n",
      "Gen Loss: 2.62199 Disc Loss: 0.245999\n",
      "Gen Loss: 2.03174 Disc Loss: 0.156082\n",
      "Gen Loss: 1.81271 Disc Loss: 0.243544\n",
      "Gen Loss: 5.10974 Disc Loss: 0.136236\n",
      "Gen Loss: 2.74618 Disc Loss: 0.189005\n",
      "Gen Loss: 2.97279 Disc Loss: 0.197411\n",
      "Gen Loss: 2.69416 Disc Loss: 0.139372\n",
      "Gen Loss: 3.03906 Disc Loss: 0.109543\n",
      "Gen Loss: 1.54715 Disc Loss: 0.114617\n",
      "Gen Loss: 2.41149 Disc Loss: 0.244156\n",
      "Gen Loss: 4.62084 Disc Loss: 0.39568\n",
      "Gen Loss: 3.93009 Disc Loss: 0.21738\n",
      "Gen Loss: 3.32776 Disc Loss: 0.120001\n",
      "Gen Loss: 3.2821 Disc Loss: 0.149861\n",
      "Gen Loss: 4.0388 Disc Loss: 0.0893207\n",
      "Gen Loss: 2.33087 Disc Loss: 0.168667\n",
      "Gen Loss: 3.78303 Disc Loss: 0.115941\n",
      "Gen Loss: 4.86891 Disc Loss: 0.0648714\n",
      "Gen Loss: 3.24463 Disc Loss: 0.159777\n",
      "Saved Model\n",
      "Gen Loss: 2.20453 Disc Loss: 0.302442\n",
      "Gen Loss: 4.04628 Disc Loss: 0.253253\n",
      "Gen Loss: 3.11091 Disc Loss: 0.120953\n",
      "Gen Loss: 3.85359 Disc Loss: 0.062217\n",
      "Gen Loss: 3.02604 Disc Loss: 0.0994124\n",
      "Gen Loss: 4.86399 Disc Loss: 0.0687811\n",
      "Gen Loss: 3.24546 Disc Loss: 0.154921\n",
      "Gen Loss: 0.233933 Disc Loss: 1.96581\n",
      "Gen Loss: 3.30785 Disc Loss: 0.211412\n",
      "Gen Loss: 3.45992 Disc Loss: 0.12798\n",
      "Gen Loss: 6.21773 Disc Loss: 0.437772\n",
      "Gen Loss: 2.20141 Disc Loss: 0.176321\n",
      "Gen Loss: 2.94772 Disc Loss: 0.200664\n",
      "Gen Loss: 2.93067 Disc Loss: 0.132788\n",
      "Gen Loss: 3.94827 Disc Loss: 0.0597479\n",
      "Gen Loss: 3.53492 Disc Loss: 0.0714127\n",
      "Gen Loss: 3.09564 Disc Loss: 0.154424\n",
      "Gen Loss: 6.77007 Disc Loss: 0.342053\n",
      "Gen Loss: 3.64332 Disc Loss: 0.14895\n",
      "Gen Loss: 1.59915 Disc Loss: 0.399458\n",
      "Gen Loss: 2.83587 Disc Loss: 0.17255\n",
      "Gen Loss: 3.06554 Disc Loss: 0.135926\n",
      "Gen Loss: 3.0893 Disc Loss: 0.353776\n",
      "Gen Loss: 4.08155 Disc Loss: 0.239364\n",
      "Gen Loss: 3.50521 Disc Loss: 0.0736871\n",
      "Gen Loss: 2.53944 Disc Loss: 0.146341\n",
      "Gen Loss: 5.90922 Disc Loss: 0.202176\n",
      "Gen Loss: 2.16367 Disc Loss: 0.130005\n",
      "Gen Loss: 2.50023 Disc Loss: 0.0714041\n",
      "Gen Loss: 4.32704 Disc Loss: 0.0975949\n",
      "Gen Loss: 2.68379 Disc Loss: 0.0335292\n",
      "Gen Loss: 4.61833 Disc Loss: 0.123689\n",
      "Gen Loss: 4.97087 Disc Loss: 0.190942\n",
      "Gen Loss: 3.08381 Disc Loss: 0.0467219\n",
      "Gen Loss: 4.87469 Disc Loss: 0.177912\n",
      "Gen Loss: 2.03082 Disc Loss: 0.150105\n",
      "Gen Loss: 3.39799 Disc Loss: 0.0693123\n",
      "Gen Loss: 4.09076 Disc Loss: 0.100331\n",
      "Gen Loss: 2.30324 Disc Loss: 0.0791057\n",
      "Gen Loss: 2.75497 Disc Loss: 0.138883\n",
      "Gen Loss: 3.88622 Disc Loss: 0.178878\n",
      "Gen Loss: 2.22814 Disc Loss: 8.06149\n",
      "Gen Loss: 2.35185 Disc Loss: 0.293354\n",
      "Gen Loss: 3.14092 Disc Loss: 0.382631\n",
      "Gen Loss: 2.81547 Disc Loss: 0.559904\n",
      "Gen Loss: 2.18316 Disc Loss: 0.324436\n",
      "Gen Loss: 3.28175 Disc Loss: 0.18557\n",
      "Gen Loss: 3.19588 Disc Loss: 0.178874\n",
      "Gen Loss: 3.25665 Disc Loss: 0.129204\n",
      "Gen Loss: 3.54883 Disc Loss: 0.096299\n",
      "Gen Loss: 3.24414 Disc Loss: 0.111057\n",
      "Gen Loss: 2.66816 Disc Loss: 0.173071\n",
      "Gen Loss: 3.41469 Disc Loss: 0.11645\n",
      "Gen Loss: 3.60702 Disc Loss: 0.146839\n",
      "Gen Loss: 2.85219 Disc Loss: 0.217591\n",
      "Gen Loss: 2.91219 Disc Loss: 0.166776\n",
      "Gen Loss: 7.40028 Disc Loss: 0.718497\n",
      "Gen Loss: 2.95587 Disc Loss: 0.196422\n",
      "Gen Loss: 4.43409 Disc Loss: 0.0880097\n",
      "Gen Loss: 3.06899 Disc Loss: 0.312254\n",
      "Gen Loss: 4.00776 Disc Loss: 0.137861\n",
      "Gen Loss: 9.69939 Disc Loss: 0.499981\n",
      "Gen Loss: 4.10532 Disc Loss: 0.0576618\n",
      "Gen Loss: 3.93152 Disc Loss: 0.0914247\n",
      "Gen Loss: 3.57936 Disc Loss: 0.111957\n",
      "Gen Loss: 4.06595 Disc Loss: 0.128282\n",
      "Gen Loss: 4.85325 Disc Loss: 0.0925854\n",
      "Gen Loss: 3.03338 Disc Loss: 0.103094\n",
      "Gen Loss: 3.61167 Disc Loss: 0.11571\n",
      "Gen Loss: 4.69911 Disc Loss: 0.0916332\n",
      "Gen Loss: 4.16297 Disc Loss: 0.1192\n",
      "Gen Loss: 2.62863 Disc Loss: 0.128769\n",
      "Gen Loss: 2.48001 Disc Loss: 0.0994029\n",
      "Gen Loss: 4.69678 Disc Loss: 0.415071\n",
      "Gen Loss: 5.04654 Disc Loss: 0.0753517\n",
      "Gen Loss: 3.68419 Disc Loss: 0.0955825\n",
      "Gen Loss: 4.07648 Disc Loss: 0.0317747\n",
      "Gen Loss: 3.68092 Disc Loss: 0.0568041\n",
      "Gen Loss: 3.22111 Disc Loss: 0.117541\n",
      "Gen Loss: 2.2668 Disc Loss: 0.198285\n",
      "Gen Loss: 2.66633 Disc Loss: 0.262326\n",
      "Gen Loss: 3.58517 Disc Loss: 0.096237\n",
      "Gen Loss: 3.18583 Disc Loss: 0.127323\n",
      "Gen Loss: 3.51168 Disc Loss: 0.132136\n",
      "Gen Loss: 2.74779 Disc Loss: 7.62304\n",
      "Gen Loss: 3.9705 Disc Loss: 0.360485\n",
      "Gen Loss: 2.97294 Disc Loss: 0.202103\n",
      "Gen Loss: 4.19517 Disc Loss: 0.224428\n",
      "Gen Loss: 3.28564 Disc Loss: 0.159942\n",
      "Gen Loss: 2.91517 Disc Loss: 0.158727\n",
      "Gen Loss: 3.0908 Disc Loss: 0.130231\n",
      "Gen Loss: 3.06786 Disc Loss: 0.170318\n",
      "Gen Loss: 3.16562 Disc Loss: 0.173496\n",
      "Gen Loss: 3.67929 Disc Loss: 0.182099\n",
      "Gen Loss: 2.14021 Disc Loss: 0.0545575\n",
      "Gen Loss: 7.07963 Disc Loss: 0.586119\n",
      "Gen Loss: 3.47027 Disc Loss: 0.208818\n",
      "Gen Loss: 3.36878 Disc Loss: 0.153453\n",
      "Gen Loss: 4.71982 Disc Loss: 0.247494\n",
      "Gen Loss: 3.45169 Disc Loss: 0.430423\n",
      "Saved Model\n",
      "Gen Loss: 2.07605 Disc Loss: 0.217197\n",
      "Gen Loss: 3.3862 Disc Loss: 0.186611\n",
      "Gen Loss: 3.87251 Disc Loss: 0.0891122\n",
      "Gen Loss: 4.41569 Disc Loss: 0.19871\n",
      "Gen Loss: 4.18182 Disc Loss: 0.0596293\n",
      "Gen Loss: 3.03298 Disc Loss: 0.0591625\n",
      "Gen Loss: 2.84935 Disc Loss: 0.104677\n",
      "Gen Loss: 3.75453 Disc Loss: 0.0701431\n",
      "Gen Loss: 3.11012 Disc Loss: 0.0735655\n",
      "Gen Loss: 4.34768 Disc Loss: 0.0916785\n",
      "Gen Loss: 3.88851 Disc Loss: 0.121143\n",
      "Gen Loss: 3.2918 Disc Loss: 0.0987362\n",
      "Gen Loss: 3.90116 Disc Loss: 0.0402928\n",
      "Gen Loss: 3.77271 Disc Loss: 0.0410592\n",
      "Gen Loss: 3.58878 Disc Loss: 0.0666202\n",
      "Gen Loss: 3.53483 Disc Loss: 0.0681471\n",
      "Gen Loss: 3.64965 Disc Loss: 0.0613101\n",
      "Gen Loss: 3.25767 Disc Loss: 0.149617\n",
      "Gen Loss: 6.30185 Disc Loss: 0.0374261\n",
      "Gen Loss: 3.6856 Disc Loss: 0.068526\n",
      "Gen Loss: 4.30086 Disc Loss: 0.0383901\n",
      "Gen Loss: 3.75025 Disc Loss: 0.0189852\n",
      "Gen Loss: 3.47223 Disc Loss: 0.0456091\n",
      "Gen Loss: 3.78016 Disc Loss: 0.0516122\n",
      "Gen Loss: 4.58871 Disc Loss: 0.0386608\n",
      "Gen Loss: 3.23004 Disc Loss: 0.0358752\n",
      "Gen Loss: 4.29092 Disc Loss: 0.0169696\n",
      "Gen Loss: 3.76501 Disc Loss: 0.070301\n",
      "Gen Loss: 3.82616 Disc Loss: 0.0774562\n",
      "Gen Loss: 4.06068 Disc Loss: 0.0569078\n",
      "Gen Loss: 5.08997 Disc Loss: 0.242849\n",
      "Gen Loss: 5.60112 Disc Loss: 0.684305\n",
      "Gen Loss: 2.84383 Disc Loss: 0.239179\n",
      "Gen Loss: 1.59501 Disc Loss: 0.909386\n",
      "Gen Loss: 4.23226 Disc Loss: 0.444884\n",
      "Gen Loss: 3.27915 Disc Loss: 0.128374\n",
      "Gen Loss: 4.85622 Disc Loss: 0.179277\n",
      "Gen Loss: 2.28921 Disc Loss: 0.185652\n",
      "Gen Loss: 3.58938 Disc Loss: 0.133677\n",
      "Gen Loss: 3.75732 Disc Loss: 0.180926\n",
      "Gen Loss: 3.5448 Disc Loss: 0.120877\n",
      "Gen Loss: 3.85938 Disc Loss: 0.124187\n",
      "Gen Loss: 3.81137 Disc Loss: 0.260419\n",
      "Gen Loss: 4.03842 Disc Loss: 0.0496406\n",
      "Gen Loss: 4.34607 Disc Loss: 0.0600779\n",
      "Gen Loss: 3.79789 Disc Loss: 0.111188\n",
      "Gen Loss: 4.11305 Disc Loss: 0.154914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 5.6524 Disc Loss: 0.151292\n",
      "Gen Loss: 3.89483 Disc Loss: 0.118637\n",
      "Gen Loss: 3.28015 Disc Loss: 0.0956236\n",
      "Gen Loss: 3.1038 Disc Loss: 0.100689\n",
      "Gen Loss: 3.27578 Disc Loss: 0.0921635\n",
      "Gen Loss: 3.51649 Disc Loss: 0.114802\n",
      "Gen Loss: 3.49099 Disc Loss: 0.113083\n",
      "Gen Loss: 2.57808 Disc Loss: 0.16921\n",
      "Gen Loss: 3.32895 Disc Loss: 0.187755\n",
      "Gen Loss: 2.41758 Disc Loss: 0.128444\n",
      "Gen Loss: 4.4423 Disc Loss: 0.207484\n",
      "Gen Loss: 3.15709 Disc Loss: 0.154883\n",
      "Gen Loss: 3.07746 Disc Loss: 0.149246\n",
      "Gen Loss: 1.83112 Disc Loss: 0.168082\n",
      "Gen Loss: 5.61665 Disc Loss: 0.227868\n",
      "Gen Loss: 7.27854 Disc Loss: 0.278523\n",
      "Gen Loss: 3.13636 Disc Loss: 0.0439358\n",
      "Gen Loss: 2.16522 Disc Loss: 0.145151\n",
      "Gen Loss: 3.85197 Disc Loss: 0.0608563\n",
      "Gen Loss: 4.88589 Disc Loss: 0.0540174\n",
      "Gen Loss: 4.75121 Disc Loss: 0.0634622\n",
      "Gen Loss: 4.15148 Disc Loss: 0.0458548\n",
      "Gen Loss: 3.64053 Disc Loss: 0.146439\n",
      "Gen Loss: 3.18938 Disc Loss: 0.104549\n",
      "Gen Loss: 7.21518 Disc Loss: 0.636701\n",
      "Gen Loss: 3.04527 Disc Loss: 0.191215\n",
      "Gen Loss: 6.0297 Disc Loss: 0.0397139\n",
      "Gen Loss: 3.70171 Disc Loss: 0.102601\n",
      "Gen Loss: 3.48381 Disc Loss: 0.103504\n",
      "Gen Loss: 2.45966 Disc Loss: 0.143351\n",
      "Gen Loss: 3.27479 Disc Loss: 0.0591732\n",
      "Gen Loss: 2.48654 Disc Loss: 0.0616984\n",
      "Gen Loss: 4.1739 Disc Loss: 0.227916\n",
      "Gen Loss: 3.21168 Disc Loss: 0.16429\n",
      "Gen Loss: 3.01459 Disc Loss: 0.0899716\n",
      "Gen Loss: 5.09661 Disc Loss: 0.26888\n",
      "Gen Loss: 2.55072 Disc Loss: 0.816151\n",
      "Gen Loss: 3.85945 Disc Loss: 0.138887\n",
      "Gen Loss: 3.57464 Disc Loss: 0.109269\n",
      "Gen Loss: 2.67794 Disc Loss: 0.0629398\n",
      "Gen Loss: 1.17387 Disc Loss: 0.627989\n",
      "Gen Loss: 3.29071 Disc Loss: 0.311148\n",
      "Gen Loss: 3.57114 Disc Loss: 0.330423\n",
      "Gen Loss: 2.61257 Disc Loss: 0.535878\n",
      "Gen Loss: 2.40203 Disc Loss: 1.10496\n",
      "Gen Loss: 4.54655 Disc Loss: 0.144802\n",
      "Gen Loss: 4.40523 Disc Loss: 0.0633375\n",
      "Gen Loss: 2.93518 Disc Loss: 0.246581\n",
      "Gen Loss: 4.20138 Disc Loss: 0.123877\n",
      "Gen Loss: 3.12098 Disc Loss: 0.0942822\n",
      "Gen Loss: 2.58702 Disc Loss: 0.169269\n",
      "Gen Loss: 4.63048 Disc Loss: 0.0970106\n",
      "Gen Loss: 2.76366 Disc Loss: 0.0839476\n",
      "Saved Model\n",
      "Gen Loss: 3.79088 Disc Loss: 0.0989434\n",
      "Gen Loss: 3.13085 Disc Loss: 0.0889075\n",
      "Gen Loss: 4.59169 Disc Loss: 0.161066\n",
      "Gen Loss: 2.97714 Disc Loss: 0.174191\n",
      "Gen Loss: 3.10282 Disc Loss: 0.238232\n",
      "Gen Loss: 3.99834 Disc Loss: 0.0795107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bf1dbd0c3024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreal_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator, twice for good measure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gen Loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgLoss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Disc Loss: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1109\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mdirect\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \"\"\"\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2924\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m     \"\"\"\n\u001b[0;32m-> 2926\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_controller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2928\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Issue 19330: ensure context manager instances have good docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 500000 #Total number of iterations to use.\n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in tqdm.tqdm_notebook(range(iterations)):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
    "        if i % 10 == 0: \n",
    "            print(\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-7000.cptk\n"
     ]
    }
   ],
   "source": [
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "batch_size_sample = 36\n",
    "\n",
    "path = model_directory  + '/model-7000.cptk'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print( 'Loading Model...')\n",
    "    saver.restore(sess, save_path=path)\n",
    "    #     ckpt = tf.train.get_checkpoint_state(model_directory)\n",
    "    #     saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    zs = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32) #Generate a random z batch\n",
    "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6],sample_directory+'/fig.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (2 points)\n",
    "Run a couple of iterations and visualize examples generated by the generator (Could be found in ./fig folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMAAAADACAAAAAB3tzPbAAAmG0lEQVR4nM19Z4AcxdH2Uz0zmy5n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\nSUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='figs/fig.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (5 points)\n",
    "Evaluate discrimator accuracy in the pre-trained model on any representative subsample of fashion-minst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 1000 #Total number of iterations to use.\n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "path = model_directory + '/model-7000.cptk'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "r = 0\n",
    "cnt_rght=0\n",
    "cnt_wrng=0\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    saver.restore(sess, save_path=path)\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        xs,_ = mnist.test.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        \n",
    "        \n",
    "        dx, dg = sess.run([Dx, Dg],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        res = sess.run(Dx, feed_dict={real_in: xs})\n",
    "        print(dx)\n",
    "        r += (dx >= 0.1)\n",
    "        r += (dg <= 0.1)\n",
    "        print('l', len(dx))\n",
    "        print(cnt_rght)\n",
    "\n",
    "        for j in range(len(dx)):\n",
    "            if dx[j] > 0.1:\n",
    "                cnt_rght += 1\n",
    "            else:\n",
    "                cnt_wrng += 1\n",
    "        \n",
    "        for j in range(len(dg)):\n",
    "            if dg[j] < 0.1:\n",
    "                cnt_rght += 1\n",
    "            else:\n",
    "                cnt_wrng += 1\n",
    "        print(cnt_rght)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"accuracy = \", cnt_rght/(cnt_rght + cnt_wrng), r/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
